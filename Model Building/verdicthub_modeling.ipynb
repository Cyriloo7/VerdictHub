{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4UyLNBCWnl63",
   "metadata": {
    "id": "4UyLNBCWnl63"
   },
   "source": [
    "# verdicthub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "onPLo6Ilnovd",
   "metadata": {
    "id": "onPLo6Ilnovd"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u_xMX7llKcWl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_xMX7llKcWl",
    "is_executing": true,
    "outputId": "96d802d6-eee7-4b73-f92e-ec8c4a0bee5d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"C:\\Users\\cyril\\Pictures\\GAIP\\Project\\Dataset\\Judgment_minimal.csv\")  # Assuming data is in a CSV file\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "OWWRZ2wNg9p7",
   "metadata": {
    "id": "OWWRZ2wNg9p7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"BACKGROUND TO THE APPEAL\"], data[\"JUDGMENT_MINIMAL\"], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "IOaGepAZ3BNr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "IOaGepAZ3BNr",
    "outputId": "9d542e4b-9755-4d91-97f9-e4e42fbe661c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'lord neuberger president lord mance lord clarke lord sumption lord hodge background appeal appeal arises fact foundation structure two offshore wind farm robin rigg solway firth designed installed respondent mt hjgaard mth failed shortly companypletion project dispute companycerns bear remedial companyts sum may appellant two companypanies eon group eon sent tender document various party including mth due companyrse became successful bidder tender document included eon technical requirement technical requirement laid minimum requirement taken account companytractor ie ultimately mth amongst thin g technical requirement called foundation accordance document known j j reference international standard design offshore wind turbine published independent classification certification agency j provides certain mathematical formula calculate aspect foundation structure one formula included given specific value later review showed value given wrong factor ten error meant strength foundation structure substantially estimated selected companytractor work mth duly set preparing tender accordance eon requirement j finally eon mth entered companytract mth agreed design fabricate install foundation proposed turbine clause x companytract stated mth carry work shall fit purpose fit purpose ultimately defined way included adherence technical requirement party agreed carry remedial work immediately foundation structure started failing proceeding companycern question bear companyt remedial work high court found eon clause x companytract ultimately referred technical requirement para technical requirement another provision para b required foundation designed would lifetime twenty year number mth held liable appeal court appeal found otherwise inconsistency para para b technical requirement one hand companytractual provision particular adherence j hand court appeal ruled companytractual provision prevail'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996ea96e4ae4bbf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:40:06.062509100Z",
     "start_time": "2023-12-27T06:40:01.307462Z"
    },
    "id": "996ea96e4ae4bbf1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Load and preprocess data\n",
    "#data = pd.read_csv(r\"/content/Judgment_minimal.csv\")  # Assuming data is in a CSV file\n",
    "#data = data.dropna()\n",
    "\n",
    "#df[\"BACKGROUND TO THE APPEAL\"] = data[\"BACKGROUND TO THE APPEAL\"].apply(clean_text)\n",
    "#df[\"JUDGMENT\"] = data[\"JUDGMENT\"].apply(clean_text)\n",
    "\n",
    "#data[\"BACKGROUND TO THE APPEAL\"] = data[\"BACKGROUND TO THE APPEAL\"].apply(str.lower)  # Example preprocessing\n",
    "#data[\"ORIGINAL JUDGMENT\"] = data[\"ORIGINAL JUDGMENT\"].apply(str.lower)\n",
    "\n",
    "# Prepare input and target examples\n",
    "input_texts = X_train #data[\"BACKGROUND TO THE APPEAL\"].tolist()\n",
    "target_texts = y_train #data[\"JUDGMENT_MINIMAL\"].tolist()\n",
    "\n",
    "# Create DataLoader with appropriate batch size\n",
    "batch_size = 1  # Adjust as needed\n",
    "data_loader = DataLoader(list(zip(input_texts, target_texts)), batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2j6AGWkTbQj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2j6AGWkTbQj",
    "outputId": "1c7d0f8f-6e10-43f7-af6f-e8b89af88c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pQtNX93GuTQo",
   "metadata": {
    "id": "pQtNX93GuTQo"
   },
   "source": [
    "### t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce26a4cd31dc6b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:30:28.935672200Z",
     "start_time": "2023-12-27T06:30:19.351858500Z"
    },
    "id": "3ce26a4cd31dc6b2"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44bdc2d53474040d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:31:15.491219100Z",
     "start_time": "2023-12-27T06:30:28.936670200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44bdc2d53474040d",
    "outputId": "d0a6fef2-5423-4e37-f918-25290caead3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-small\"  # Choose a suitable transformer model\n",
    "#model_name = \"t5-large\"\n",
    "#model_name = \"eleutheral/falcon-7b\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ZUeVdT9kTdo",
   "metadata": {
    "id": "3ZUeVdT9kTdo"
   },
   "source": [
    "### Pegasus-xsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "R2Sm6dXhIHuX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2Sm6dXhIHuX",
    "outputId": "71ae0950-0474-4983-a138-113acf5b7a74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name, max_length = 1024)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F0KiquijDwt4",
   "metadata": {
    "id": "F0KiquijDwt4"
   },
   "source": [
    "### Models Tryed to run in system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7340c095af3268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T15:23:40.576949100Z",
     "start_time": "2023-12-26T15:16:41.931627Z"
    },
    "id": "9a7340c095af3268"
   },
   "outputs": [],
   "source": [
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d943c818656e77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T01:05:58.097715800Z",
     "start_time": "2023-12-27T01:05:44.403208800Z"
    },
    "id": "a8d943c818656e77"
   },
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"nsi319/legal-pegasus\")  # Adjust model name as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e1f5c2b90f58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T23:50:03.596495600Z",
     "start_time": "2023-12-26T23:49:47.775938400Z"
    },
    "id": "7f4e1f5c2b90f58"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-base\")  # Adjust model name as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ezuzuoOND11m",
   "metadata": {
    "id": "ezuzuoOND11m"
   },
   "source": [
    "### Gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62dfc4e6c7399ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:38:03.215897500Z",
     "start_time": "2023-12-27T06:38:03.177832300Z"
    },
    "id": "62dfc4e6c7399ac0"
   },
   "outputs": [],
   "source": [
    "# Move model to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97a2d5e79151e317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T06:38:04.601392800Z",
     "start_time": "2023-12-27T06:38:04.576037600Z"
    },
    "id": "97a2d5e79151e317"
   },
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Adjust learning rate if needed\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "X2Y21J5eRESs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2Y21J5eRESs",
    "outputId": "842b7bec-a31a-491f-b565-ab5e20c0fa4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 4.0782\n",
      "Epoch 2/20, Loss: 3.5708\n",
      "Epoch 3/20, Loss: 3.3553\n",
      "Epoch 4/20, Loss: 3.1665\n",
      "Epoch 5/20, Loss: 2.9820\n",
      "Epoch 6/20, Loss: 2.7898\n",
      "Epoch 7/20, Loss: 2.5853\n",
      "Epoch 8/20, Loss: 2.3964\n",
      "Epoch 9/20, Loss: 2.1747\n",
      "Epoch 10/20, Loss: 1.9799\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_idx, (input_text, target_text) in enumerate(data_loader):\n",
    "        # Tokenize input and target text\n",
    "        encoding = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        input_ids = encoding.input_ids\n",
    "        attention_mask = encoding.attention_mask\n",
    "\n",
    "        target_ids = tokenizer(target_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        # Move the tensors to the device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        #print(model)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(outputs.logits.view(-1, model.config.vocab_size), target_ids.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{20}, Loss: {epoch_loss / len(input_texts):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29403f3ac690dcd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29403f3ac690dcd2",
    "outputId": "c5d90ea2-2bf3-4d39-ef53-fd36fd471ba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,    37,  8531,  2243, 16708,    15,     7,     8,  3958,    57,\n",
      "            3,     9,  2942,    13,     3,    18,     6,    28,     8,   380,\n",
      "           13,  2809, 12991,   739,    16,     8,   962,    13,   823,     8,\n",
      "         3958,    19,  2016,   242,     8,  3958,     8,  1046,  1614,   398,\n",
      "          455,     3,     9,     3,    60,    17, 12042,     3,    99,    34,\n",
      "           19,  2016,    21,     8,  8319,  6761,     7,  4616,  2809,   283,\n",
      "          663,  1527,     8,   991,  7661,     6,    28,    84,  2809, 12991,\n",
      "          739,  2065,     7,  2809,   283,   663,  1527,     3,     9,     3,\n",
      "           26, 10924,    53,  7661,     3, 13223, 10466,   134,  5652,  1853,\n",
      "          446, 10161,   517, 11810,    37,     3, 27794,  6761,     7, 24062,\n",
      "           19,    59,  2303,    57,     8,     3,    75,    40,     3,    52,\n",
      "           26,     7, 24062,    11,     3,    23,    23,     8,  4616,     3,\n",
      "            6,    68,     3,    23,   208,     8,  4616,    16,    84,     8,\n",
      "         8319,  6761,     7,   130,     3, 21217,     3,     6,    11,     8,\n",
      "         1614,    19,    59, 19880,    26,    12, 17981, 15064,   348,  2084,\n",
      "           45,     8,  1614,     3,     7,  1357,     3,     6,     3,     6,\n",
      "         2809,  3899,    11,  2809, 12991,   739,    16,     8,   962,    13,\n",
      "          823,     8,  1614,    65,     3,     9,     3,  2666,  1167,    13,\n",
      "        29205,    24,     8,  1046,  1614,    19, 19880,    26,    12,   455,\n",
      "            3,     9,     3,    60,    17, 12042,     3,     6,    68,    24,\n",
      "            8,  1046,  1614,    65,     3,     9,   269,    12,   455,     3,\n",
      "            9,     3,    60,    17, 12042,     3,     6,    11,    24,     8,\n",
      "         1046,  1614,    65,     3,     9,   269,    12,   455,     3,     9,\n",
      "            3,    60,    17, 12042,     3,     6,    11,    24,     8,  1046,\n",
      "         1614,    65,     3,     9,   269,    12,   455,     3,     9,     3,\n",
      "           60,    17, 12042,     3,     6,  2809,   283,   663,  2065,     7,\n",
      "           28,  2809, 12991,   739,    16,   112,   903,    13,     8,   962,\n",
      "           13,   823,     8,  1046,  1614,    65,     3,     9,   579,    12,\n",
      "          455,     3,     9,     3,    60,    17, 12042,     3,     6,    42,\n",
      "            3,    23,    23,    23,     8,   579,    13,     8,  1046,  1614,\n",
      "           12,   455,     3,     9,     3,    60,    17, 12042,     3,     6,\n",
      "           37,  1046,  1614,    19,    59,   831,    12,   103,    78,     6,\n",
      "           68,    34,    19,    59,     3,     9,  1052,    21,     8,  1046,\n",
      "         4831,    12,    36,  5563,     3,    60,    17,  9889,    16,     3,\n",
      "            9,   194,    84,    19,    72,     3,     9,  1052,    21,     8,\n",
      "         3659,    13,  1375,    13,     8,  1983,     3,     6,     3,     6,\n",
      "            3,     6,     3,     6,     3,     6,  2809,  3899,    16,     8,\n",
      "          962,    13,   823,     8,  1046,  1614,    65,     3,     9,   269,\n",
      "           12,   455,     3,     9,     3,    60,    17, 12042,   365,  1375,\n",
      "            3,     6,    84,     8,     3, 27794,  6761,     7,  4284,   133,\n",
      "            3,     9,    75, 10073,  1054,    45,     8,  5447,     3,    99,\n",
      "           79,   130,     3, 21217,     6,     3, 10339,     8,     3, 27794,\n",
      "         6761,     7,   130,     3, 21217,    13,     8,     3, 12554, 10883,\n",
      "         2319,     3,    88,  3454,    16,     8,     3,  5840,  1152,    63,\n",
      "           16,     8,     3, 12554, 10883,  2319,     3,  7361,  5714,     3,\n",
      "           18,  2809, 12991,   739,  2065,     7,    28,     8,   903,    24,\n",
      "         1375,    13,     8,  1983,   795,   163,    12,     8,  1046,  1614,\n",
      "            8,  1004,    12,   455,     3,     9,     3,    60,    17, 12042,\n",
      "            3,     6,     3,     6,     3,     6,     3,     6,     3,     6,\n",
      "            3,     6,     3,     6,     3,     6,     3,     6,     3,     6,\n",
      "            3,     6,     3,     6,     3,     6,     3,     6,     3,     6,\n",
      "            3,     6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Case appeal: [lord rodger lord brown lord mance lord collins lord dyson background appeal issue appeal whether court ppeal right order retrial respect appellant circumstance companyrt may order retrial set section criminal appeal act amended crim inal justice act provides court appeal allows appeal companyviction appears court interest justice require may order appellant retried appellant brother companyvicted mu rder two robbery leeds crown court february appellant sentenced life imprisonment murder served companycurrent twelveyear term robbery main prosecution witness karl chapman professional criminal supergrass june october robbery took place home two elderly brother occasion th e robber used violence took money second occasion elder brother sustained injury head later resulted death ensuing police investigation chapman provided police information witness statement implicating appellant brother charged robbery murder chapman evidence central prosecution case tria l vigorously denied expecting receiving benefit police evidence following companyvictions allegation local press police planning pay chapman large sum money upon release prison subsequently crim inal case review commission ccrc decided investigate north yorkshire police carried detaile investigation activity police formed basis ccrc report nove mber finding report showed police companyspired pervert companyrse justice companycealing lying variety reward benefit received chapman vealed example police paid sum money taken brothel allowed companysume drug companypany number investigated allegation companymitted viol ent attack november ccrc made reference criminal divisi court appeal ground companyvictions procured gross prosecutorial misconduct part police appellant brother remained prison period october september whilst prison appellant made series admission guilt freely voluntarily various person december court appeal quashed companyvictions appellant brother finding ccrc relating gross po louse misconduct number challenged companyrt held finding revealed trial trial judge might stayed prosecution abuse process applied section th e police criminal evidence act exclude chapman evidence altogether case th e appellant brother would acquitted however companyrt also held ad mission made appellant companystituted clear companypelling evidence gu ilt light court appeal found interest justice order retrial appellant supreme court united kingdom parliament square london swp bd f wwwsupremecourtgovuk]\\nVerdict:\"\n",
    "input_ids = tokenizer.encode(input_prompt,truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "input_ids = input_ids.to(model.device)\n",
    "\n",
    "generated_verdict = model.generate(input_ids, max_length=512, num_return_sequences=1)[0]\n",
    "print(generated_verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7535f75a41bdaa6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7535f75a41bdaa6e",
    "outputId": "17c041e8-2cf1-4aba-b7fd-a9ca006290d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Supreme Court dismisses the appeal by a majority of -, with the support of Lord Dyson in the issue of whether the appeal is appropriate For the appeal the interest court must order a retrial if it is appropriate for the appellants circumstances Lord Mance gives the lead judgment, with which Lord Dyson agrees Lord Mance gives a dissenting judgment REASONS FOR THE JUDGMENT The Appellants circumstance is not covered by the cl rds circumstance and ii the circumstances, but iv the circumstances in which the appellants were convicted, and the court is not obliged to exclude chapman evidence from the court s decision,, Lord Brown and Lord Dyson in the issue of whether the court has a trump of contention that the interest court is obliged to order a retrial, but that the interest court has a right to order a retrial, and that the interest court has a right to order a retrial, and that the interest court has a right to order a retrial, Lord Mance agrees with Lord Dyson in his view of the issue of whether the interest court has a power to order a retrial, or iii the power of the interest court to order a retrial, The interest court is not required to do so, but it is not a matter for the interest justice to be ordered retried in a way which is more a matter for the purposes of section of the Act,,,,, Lord Brown in the issue of whether the interest court has a right to order a retrial under section, which the Appellants brother would acquitted from the crime if they were convicted, whereas the Appellants were convicted of the alleged offences he committed in the robbery in the alleged offences remained prison - Lord Dyson agrees with the view that section of the Act provides only to the interest court the opportunity to order a retrial,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "# Convert the tensor back to CPU if necessary\n",
    "decoded_text = tokenizer.decode(generated_verdict.cpu(), skip_special_tokens=True)\n",
    "\n",
    "# Print the generated verdict\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1pEsHKYFjqH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1pEsHKYFjqH",
    "outputId": "e50c420d-39c3-4a2c-a41d-dd35daa7d5aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/t5-small-model/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/t5-small-model/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/t5-small-model/spiece.model',\n",
       " '/content/drive/MyDrive/t5-small-model/added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('/content/drive/MyDrive/t5-small-model')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/t5-small-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R-pVxAUZ8m8F",
   "metadata": {
    "id": "R-pVxAUZ8m8F"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mFCS44bJzPKy",
   "metadata": {
    "id": "mFCS44bJzPKy"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "RkJmpug6gGcM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkJmpug6gGcM",
    "outputId": "757addba-71d1-4d93-9ab0-757dd72fefdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/T5-small_model/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/T5-small_model/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/T5-small_model/spiece.model',\n",
       " '/content/drive/MyDrive/T5-small_model/added_tokens.json',\n",
       " '/content/drive/MyDrive/T5-small_model/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the directory where you want to save the model\n",
    "output_dir = \"/content/drive/MyDrive/T5-small_model\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "qyWV1lyftlak",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyWV1lyftlak",
    "outputId": "c0246beb-8a41-48e4-cbf0-47f3eaa0aac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements (1).txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements (1).txt (line 2)) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements (1).txt (line 3)) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements (1).txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r /content/requirements (1).txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r /content/requirements (1).txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r /content/requirements (1).txt (line 2)) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r /content/requirements (1).txt (line 2)) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"/content/requirements (1).txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "AH28MckZt28o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AH28MckZt28o",
    "outputId": "abd71c5a-85a5-4d52-e334-33acf2f1969a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.1)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=daacfaa2eca8375eedfdc89e0348f436cc6dc562abb88c25197d0df73375c0b4\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dSctw3Fl35cS",
   "metadata": {
    "id": "dSctw3Fl35cS"
   },
   "source": [
    "### Testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dVOOLHgG3_iq",
   "metadata": {
    "id": "dVOOLHgG3_iq"
   },
   "source": [
    "#### Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "PQvmB-COt-aK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQvmB-COt-aK",
    "outputId": "98851f74-12b0-49b8-ca64-cbc2de8c6420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.8230088495575221, recall=0.533715925394548, fmeasure=0.6475195822454308), 'rouge2': Score(precision=0.6319290465631929, recall=0.40948275862068967, fmeasure=0.4969485614646906), 'rougeL': Score(precision=0.6902654867256637, recall=0.44763271162123386, fmeasure=0.5430809399477807), 'rougeLsum': Score(precision=0.6902654867256637, recall=0.44763271162123386, fmeasure=0.5430809399477807)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "scorer=rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL','rougeLsum'])\n",
    "scores = scorer.score(' The Supreme Court dismisses the appeal by a majori ty of Lord Dyson gives the lead judgment Lords Rodger and Mance give short concurring judgments Lords Brown and Collins dissent On November the Supreme Court handed down its decision but withheld its reasoning until the completion of the retrial On June , in Leeds Crown Court Paul Maxwell pleaded guilty REASONS FOR THE JUDGMENT By section of the Criminal Appeal Act , as am ended, Parliament has given the Criminal Division of the Court of Appeal the power to order a retrial where, having rega rd to all the circumstances of the particular case, in its view the interests of justice so require The interests of justice is not a hard-edged concept Rather, it requires an exercise of ju dgment in which a number of relevant factors have to be balanced against each other A decision of th e Court of Appeal as to whether the interests of justice require a retrial should only be upset on appeal if it was plainly wrong in the sense that it is one which no reasonable court could have made or if the court took into account immaterial factors or failed to take into account material factors - The majority of cases under section of the Act do not involve any issue of prosecutorial misconduct Indeed, no case was cited where the court had to consider the relevance of prosecutori al misconduct in the original proceedings to the question of whether the interests of justice requir e a retrial Usually, under section the court will consider the gravity of the offence, the length of time the appellant is likely to serve in custody if reconvicted, the appellants age and health, and th e wishes of the victim of the alleged offence Where prosecutorial misconduct is involved the Court of Appeal may treat the case as to some extent analogous to an application to stay proceedings as an abuse of process where it offends the courts sense of justice and propriety to try the accused However, the tests for when the court should stay proceedings for abuse of process and when it shou ld order a retrial are not coterminous The question of whether the interests of justice require a retrial is broader than the considerations involved in an application for a stay , It is common ground that the prosecutions case at a retrial would not be based on any evidence which was the product of the misconduct However, the new evidence constitutes admissions made by the appellant which would not have been made but for the original misconduct which led to his conviction The Court of Appeal was right to consider that the but for factor was no more than a relevant factor and that it was not determinative of the question whether a retrial was required in the interests of justice In deciding whether to order a retrial, there were several relevant factors which had to be weighed in the balance The balancing act is fact-sensitive and ultimately requires an exercise of judgment The Court of Appeal carri ed out the balancing exercise preci sely and with great care They held that there were strong reasons for not orderi ng a retrial given the egregious misconduct by the police However, they concluded that the public interes t in convicting those guilty of murder prevailed on the facts of this case In particular this was be cause of the gravity of the alleged offence and the existence of new and compelling evidence untainted by the police misconduct The fact that a differently constituted Court of Appeal might have come to a different conclusion is not material Accordingly, the decision of the Court of Appeal was not plainly wrong and its judgment should not be interfered with -, -, - Lord Brown, with whom Lord Collins agrees, would have allowed the appeal They would have held that since the appellant would not have made the admissions but for the prosecutorial misconduct and in light of the enormity of the police misconduct, it is inappropriate that that the case should be retried on new evidence - References in square brackets are to paragraph numbers in the judgment  ',\n",
    "                      'The Supreme Court dismisses the appeal by a majori ty of Lord Dyson gives the lead judgment Lords Rodger and Mance give short concurring judgments Lords Brown and Collins dissent On November the Supreme Court handed down its decision but withheld its reasoning until the completion of the retrial On June, in Leeds Crown Court Paul Maxwell pleaded guilty REASONS FOR THE JUDGMENT By section of the Criminal Appeal Act, as am ended, Parliament has given the Criminal Division of the Court of Appeal the power to order a retrial where, having rega rd to all the circumstances of the particular case, in its view the interests of justice so require The interests of justice is not a hard-edged concept Rather, it requires an exercise of ju dgment in which a number of relevant factors have to be balanced against each other A decision of th e Court of Appeal as to whether the interests of justice require a retrial should only be upset on appeal if it was plainly wrong in the sense that it is one which no reasonable court could have made or if the court took into account immaterial factors or failed to take into account material factors - The majority of cases under section of the Act do not involve any issue of prosecutorial misconduct Indeed, no case was cited where the court had to stay proceedings simply because the prosecution or the defence argued that the judge should stay out of it Alleg ations of prosecutorial misconduct are, however, relevant to the question of whether the interests of justice require a stay of proceedings where, having rega rd to all the circumstances of the particular case, it would not be right for the court to stay out of a retrial where, having rega rd to all the circumstances of the particular case, the court would not be right to stay out of it No case was cited where the court might stay proceedings in the interests of justice where the only potentially relevant factor was the gravity of the offence committed by the defendant However, as a matter of principle the court should not stay proceedings it out of the interests of justice where, having rega rd to all the circumstances of the particular case, it would not be right to stay out of a trial in crim inal proceedings where the only potentially relevant factor was the gravity of the offence committed by the defendant However, as a matter of principle the court should not stay proceedings in crim inal proceedings where, having rega rd to all the circumstances of the particular case, it would not be right to stay proceedings in crim inal proceedings')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "inxIjTYdzuzk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inxIjTYdzuzk",
    "outputId": "2d382eaf-5047-4411-a47f-7ce71087286a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.5067024128686327, recall=0.4144736842105263, fmeasure=0.4559710494571773), 'rouge2': Score(precision=0.11290322580645161, recall=0.09230769230769231, fmeasure=0.10157194679564692), 'rougeL': Score(precision=0.2225201072386059, recall=0.18201754385964913, fmeasure=0.20024125452352234), 'rougeLsum': Score(precision=0.2225201072386059, recall=0.18201754385964913, fmeasure=0.20024125452352234)}\n",
      "{'rouge1': Score(precision=0.6153846153846154, recall=0.15568862275449102, fmeasure=0.24850657108721624), 'rouge2': Score(precision=0.20833333333333334, recall=0.05247376311844078, fmeasure=0.08383233532934131), 'rougeL': Score(precision=0.3727810650887574, recall=0.09431137724550898, fmeasure=0.15053763440860216), 'rougeLsum': Score(precision=0.3727810650887574, recall=0.09431137724550898, fmeasure=0.15053763440860216)}\n",
      "{'rouge1': Score(precision=0.6419753086419753, recall=0.07407407407407407, fmeasure=0.13282247765006386), 'rouge2': Score(precision=0.1875, recall=0.021398002853067047, fmeasure=0.0384122919334187), 'rougeL': Score(precision=0.4074074074074074, recall=0.04700854700854701, fmeasure=0.0842911877394636), 'rougeLsum': Score(precision=0.4074074074074074, recall=0.04700854700854701, fmeasure=0.0842911877394636)}\n",
      "{'rouge1': Score(precision=0.5795454545454546, recall=0.35111876075731496, fmeasure=0.4372990353697749), 'rouge2': Score(precision=0.18518518518518517, recall=0.11206896551724138, fmeasure=0.13963480128893663), 'rougeL': Score(precision=0.3096590909090909, recall=0.18760757314974183, fmeasure=0.2336548767416935), 'rougeLsum': Score(precision=0.3096590909090909, recall=0.18760757314974183, fmeasure=0.2336548767416935)}\n",
      "{'rouge1': Score(precision=0.4806201550387597, recall=0.23938223938223938, fmeasure=0.3195876288659794), 'rouge2': Score(precision=0.11398963730569948, recall=0.05670103092783505, fmeasure=0.0757314974182444), 'rougeL': Score(precision=0.22739018087855298, recall=0.11325611325611326, fmeasure=0.15120274914089346), 'rougeLsum': Score(precision=0.22739018087855298, recall=0.11325611325611326, fmeasure=0.15120274914089346)}\n",
      "{'rouge1': Score(precision=0.589041095890411, recall=0.14972144846796656, fmeasure=0.23875624652970573), 'rouge2': Score(precision=0.17582417582417584, recall=0.0445993031358885, fmeasure=0.07115063924402446), 'rougeL': Score(precision=0.3178082191780822, recall=0.0807799442896936, fmeasure=0.12881732370905052), 'rougeLsum': Score(precision=0.3178082191780822, recall=0.0807799442896936, fmeasure=0.12881732370905052)}\n",
      "{'rouge1': Score(precision=0.4913151364764268, recall=0.28695652173913044, fmeasure=0.36230558096980786), 'rouge2': Score(precision=0.11194029850746269, recall=0.06531204644412192, fmeasure=0.08249312557286893), 'rougeL': Score(precision=0.21339950372208435, recall=0.1246376811594203, fmeasure=0.15736505032021958), 'rougeLsum': Score(precision=0.21339950372208435, recall=0.1246376811594203, fmeasure=0.15736505032021958)}\n",
      "{'rouge1': Score(precision=0.6346153846153846, recall=0.18524458700882118, fmeasure=0.2867783985102421), 'rouge2': Score(precision=0.1763085399449036, recall=0.051364365971107544, fmeasure=0.07955251709136109), 'rougeL': Score(precision=0.30494505494505497, recall=0.08901363271852446, fmeasure=0.1378026070763501), 'rougeLsum': Score(precision=0.30494505494505497, recall=0.08901363271852446, fmeasure=0.1378026070763501)}\n",
      "{'rouge1': Score(precision=0.49859943977591037, recall=0.27639751552795033, fmeasure=0.3556443556443557), 'rouge2': Score(precision=0.12359550561797752, recall=0.06842923794712286, fmeasure=0.08808808808808809), 'rougeL': Score(precision=0.26330532212885155, recall=0.14596273291925466, fmeasure=0.1878121878121878), 'rougeLsum': Score(precision=0.26330532212885155, recall=0.14596273291925466, fmeasure=0.1878121878121878)}\n",
      "{'rouge1': Score(precision=0.4703196347031963, recall=0.3007299270072993, fmeasure=0.36687444345503123), 'rouge2': Score(precision=0.13501144164759726, recall=0.08625730994152046, fmeasure=0.10526315789473684), 'rougeL': Score(precision=0.2648401826484018, recall=0.16934306569343066, fmeasure=0.20658949243098843), 'rougeLsum': Score(precision=0.2648401826484018, recall=0.16934306569343066, fmeasure=0.20658949243098843)}\n",
      "{'rouge1': Score(precision=0.5835351089588378, recall=0.34281650071123754, fmeasure=0.43189964157706096), 'rouge2': Score(precision=0.1383495145631068, recall=0.0811965811965812, fmeasure=0.10233393177737883), 'rougeL': Score(precision=0.26150121065375304, recall=0.15362731152204837, fmeasure=0.19354838709677416), 'rougeLsum': Score(precision=0.26150121065375304, recall=0.15362731152204837, fmeasure=0.19354838709677416)}\n",
      "{'rouge1': Score(precision=0.5699208443271768, recall=0.3103448275862069, fmeasure=0.4018604651162791), 'rouge2': Score(precision=0.17724867724867724, recall=0.09640287769784173, fmeasure=0.12488350419384904), 'rougeL': Score(precision=0.2717678100263852, recall=0.14798850574712644, fmeasure=0.19162790697674417), 'rougeLsum': Score(precision=0.2717678100263852, recall=0.14798850574712644, fmeasure=0.19162790697674417)}\n",
      "{'rouge1': Score(precision=0.48717948717948717, recall=0.28315946348733234, fmeasure=0.3581526861451461), 'rouge2': Score(precision=0.14910025706940874, recall=0.08656716417910448, fmeasure=0.10953729933899906), 'rougeL': Score(precision=0.23846153846153847, recall=0.13859910581222057, fmeasure=0.175306314797361), 'rougeLsum': Score(precision=0.23846153846153847, recall=0.13859910581222057, fmeasure=0.175306314797361)}\n",
      "{'rouge1': Score(precision=0.4153846153846154, recall=0.42077922077922075, fmeasure=0.4180645161290322), 'rouge2': Score(precision=0.07712082262210797, recall=0.078125, fmeasure=0.0776196636481242), 'rougeL': Score(precision=0.17692307692307693, recall=0.17922077922077922, fmeasure=0.17806451612903226), 'rougeLsum': Score(precision=0.17692307692307693, recall=0.17922077922077922, fmeasure=0.17806451612903226)}\n",
      "{'rouge1': Score(precision=0.1588235294117647, recall=0.08372093023255814, fmeasure=0.10964467005076142), 'rouge2': Score(precision=0.058997050147492625, recall=0.031055900621118012, fmeasure=0.04069175991861648), 'rougeL': Score(precision=0.11470588235294117, recall=0.06046511627906977, fmeasure=0.07918781725888324), 'rougeLsum': Score(precision=0.11470588235294117, recall=0.06046511627906977, fmeasure=0.07918781725888324)}\n",
      "{'rouge1': Score(precision=0.6010230179028133, recall=0.29012345679012347, fmeasure=0.39134054954204833), 'rouge2': Score(precision=0.16666666666666666, recall=0.08034610630407911, fmeasure=0.10842368640533778), 'rougeL': Score(precision=0.30434782608695654, recall=0.1469135802469136, fmeasure=0.19816819317235637), 'rougeLsum': Score(precision=0.30434782608695654, recall=0.1469135802469136, fmeasure=0.19816819317235637)}\n",
      "{'rouge1': Score(precision=0.4725274725274725, recall=0.32575757575757575, fmeasure=0.3856502242152466), 'rouge2': Score(precision=0.10661764705882353, recall=0.07341772151898734, fmeasure=0.08695652173913043), 'rougeL': Score(precision=0.2564102564102564, recall=0.17676767676767677, fmeasure=0.2092675635276532), 'rougeLsum': Score(precision=0.2564102564102564, recall=0.17676767676767677, fmeasure=0.2092675635276532)}\n",
      "{'rouge1': Score(precision=0.5335365853658537, recall=0.30864197530864196, fmeasure=0.39106145251396646), 'rouge2': Score(precision=0.14067278287461774, recall=0.0812720848056537, fmeasure=0.10302351623740202), 'rougeL': Score(precision=0.2804878048780488, recall=0.16225749559082892, fmeasure=0.20558659217877093), 'rougeLsum': Score(precision=0.2804878048780488, recall=0.16225749559082892, fmeasure=0.20558659217877093)}\n",
      "{'rouge1': Score(precision=0.574468085106383, recall=0.2304878048780488, fmeasure=0.32898172323759794), 'rouge2': Score(precision=0.15548780487804878, recall=0.06227106227106227, fmeasure=0.08892763731473409), 'rougeL': Score(precision=0.3161094224924012, recall=0.12682926829268293, fmeasure=0.18102697998259354), 'rougeLsum': Score(precision=0.3161094224924012, recall=0.12682926829268293, fmeasure=0.18102697998259354)}\n",
      "{'rouge1': Score(precision=0.5862068965517241, recall=0.3152639087018545, fmeasure=0.4100185528756957), 'rouge2': Score(precision=0.19148936170212766, recall=0.10285714285714286, fmeasure=0.13382899628252787), 'rougeL': Score(precision=0.3076923076923077, recall=0.16547788873038516, fmeasure=0.2152133580705009), 'rougeLsum': Score(precision=0.3076923076923077, recall=0.16547788873038516, fmeasure=0.2152133580705009)}\n",
      "{'rouge1': Score(precision=0.26366559485530544, recall=0.08855291576673865, fmeasure=0.13257881972514146), 'rouge2': Score(precision=0.11935483870967742, recall=0.04, fmeasure=0.05991902834008096), 'rougeL': Score(precision=0.1832797427652733, recall=0.061555075593952485, fmeasure=0.09215844785772029), 'rougeLsum': Score(precision=0.1832797427652733, recall=0.061555075593952485, fmeasure=0.09215844785772029)}\n",
      "{'rouge1': Score(precision=0.632, recall=0.26993166287015946, fmeasure=0.37829209896249), 'rouge2': Score(precision=0.21122994652406418, recall=0.09007981755986318, fmeasure=0.12629896083133493), 'rougeL': Score(precision=0.336, recall=0.14350797266514806, fmeasure=0.2011173184357542), 'rougeLsum': Score(precision=0.336, recall=0.14350797266514806, fmeasure=0.2011173184357542)}\n",
      "{'rouge1': Score(precision=0.5238095238095238, recall=0.2589838909541512, fmeasure=0.3466003316749586), 'rouge2': Score(precision=0.13316582914572864, recall=0.06575682382133995, fmeasure=0.08803986710963455), 'rougeL': Score(precision=0.2656641604010025, recall=0.13135068153655513, fmeasure=0.17578772802653397), 'rougeLsum': Score(precision=0.2656641604010025, recall=0.13135068153655513, fmeasure=0.17578772802653397)}\n",
      "{'rouge1': Score(precision=0.6222222222222222, recall=0.14736842105263157, fmeasure=0.2382978723404255), 'rouge2': Score(precision=0.1875, recall=0.044257112750263436, fmeasure=0.07161125319693093), 'rougeL': Score(precision=0.4, recall=0.09473684210526316, fmeasure=0.15319148936170213), 'rougeLsum': Score(precision=0.4, recall=0.09473684210526316, fmeasure=0.15319148936170213)}\n",
      "{'rouge1': Score(precision=0.5521628498727735, recall=0.33487654320987653, fmeasure=0.41690682036503357), 'rouge2': Score(precision=0.14030612244897958, recall=0.08500772797527048, fmeasure=0.10587102983638112), 'rougeL': Score(precision=0.2697201017811705, recall=0.16358024691358025, fmeasure=0.20365033621517772), 'rougeLsum': Score(precision=0.2697201017811705, recall=0.16358024691358025, fmeasure=0.20365033621517772)}\n",
      "{'rouge1': Score(precision=0.519774011299435, recall=0.282642089093702, fmeasure=0.3661691542288557), 'rouge2': Score(precision=0.12181303116147309, recall=0.06615384615384616, fmeasure=0.08574277168494518), 'rougeL': Score(precision=0.2768361581920904, recall=0.15053763440860216, fmeasure=0.19502487562189055), 'rougeLsum': Score(precision=0.2768361581920904, recall=0.15053763440860216, fmeasure=0.19502487562189055)}\n",
      "{'rouge1': Score(precision=0.5672727272727273, recall=0.25161290322580643, fmeasure=0.34860335195530723), 'rouge2': Score(precision=0.20072992700729927, recall=0.0888529886914378, fmeasure=0.12318029115341546), 'rougeL': Score(precision=0.3418181818181818, recall=0.15161290322580645, fmeasure=0.2100558659217877), 'rougeLsum': Score(precision=0.3418181818181818, recall=0.15161290322580645, fmeasure=0.2100558659217877)}\n",
      "{'rouge1': Score(precision=0.5398773006134969, recall=0.19954648526077098, fmeasure=0.2913907284768212), 'rouge2': Score(precision=0.15384615384615385, recall=0.056753688989784334, fmeasure=0.08291873963515754), 'rougeL': Score(precision=0.30368098159509205, recall=0.11224489795918367, fmeasure=0.16390728476821192), 'rougeLsum': Score(precision=0.30368098159509205, recall=0.11224489795918367, fmeasure=0.16390728476821192)}\n",
      "{'rouge1': Score(precision=0.5727272727272728, recall=0.2707736389684814, fmeasure=0.36770428015564205), 'rouge2': Score(precision=0.1337386018237082, recall=0.06312769010043041, fmeasure=0.08576998050682261), 'rougeL': Score(precision=0.26666666666666666, recall=0.12607449856733524, fmeasure=0.17120622568093385), 'rougeLsum': Score(precision=0.26666666666666666, recall=0.12607449856733524, fmeasure=0.17120622568093385)}\n",
      "{'rouge1': Score(precision=0.5375375375375375, recall=0.30338983050847457, fmeasure=0.38786565547128926), 'rouge2': Score(precision=0.1355421686746988, recall=0.07640067911714771, fmeasure=0.09771986970684039), 'rougeL': Score(precision=0.26126126126126126, recall=0.14745762711864407, fmeasure=0.18851570964247022), 'rougeLsum': Score(precision=0.26126126126126126, recall=0.14745762711864407, fmeasure=0.18851570964247022)}\n",
      "{'rouge1': Score(precision=0.5209424083769634, recall=0.32892561983471075, fmeasure=0.403242147922999), 'rouge2': Score(precision=0.17060367454068243, recall=0.1076158940397351, fmeasure=0.1319796954314721), 'rougeL': Score(precision=0.28534031413612565, recall=0.18016528925619835, fmeasure=0.220871327254306), 'rougeLsum': Score(precision=0.28534031413612565, recall=0.18016528925619835, fmeasure=0.220871327254306)}\n",
      "{'rouge1': Score(precision=0.4561933534743202, recall=0.39425587467362927, fmeasure=0.4229691876750701), 'rouge2': Score(precision=0.11818181818181818, recall=0.10209424083769633, fmeasure=0.1095505617977528), 'rougeL': Score(precision=0.21450151057401812, recall=0.185378590078329, fmeasure=0.19887955182072828), 'rougeLsum': Score(precision=0.21450151057401812, recall=0.185378590078329, fmeasure=0.19887955182072828)}\n",
      "{'rouge1': Score(precision=0.45268542199488493, recall=0.3320825515947467, fmeasure=0.38311688311688313), 'rouge2': Score(precision=0.11282051282051282, recall=0.08270676691729323, fmeasure=0.09544468546637744), 'rougeL': Score(precision=0.20971867007672634, recall=0.15384615384615385, fmeasure=0.17748917748917747), 'rougeLsum': Score(precision=0.20971867007672634, recall=0.15384615384615385, fmeasure=0.17748917748917747)}\n",
      "{'rouge1': Score(precision=0.552863436123348, recall=0.39280125195618154, fmeasure=0.45928636779505944), 'rouge2': Score(precision=0.16335540838852097, recall=0.11598746081504702, fmeasure=0.13565536205316223), 'rougeL': Score(precision=0.2577092511013216, recall=0.18309859154929578, fmeasure=0.21408966148215922), 'rougeLsum': Score(precision=0.2577092511013216, recall=0.18309859154929578, fmeasure=0.21408966148215922)}\n",
      "{'rouge1': Score(precision=0.47474747474747475, recall=0.31057268722466963, fmeasure=0.37549933422103865), 'rouge2': Score(precision=0.09121621621621621, recall=0.059602649006622516, fmeasure=0.07209612817089453), 'rougeL': Score(precision=0.2356902356902357, recall=0.15418502202643172, fmeasure=0.18641810918774965), 'rougeLsum': Score(precision=0.2356902356902357, recall=0.15418502202643172, fmeasure=0.18641810918774965)}\n",
      "{'rouge1': Score(precision=0.36678200692041524, recall=0.15680473372781065, fmeasure=0.21968911917098446), 'rouge2': Score(precision=0.1076388888888889, recall=0.045925925925925926, fmeasure=0.06438213914849429), 'rougeL': Score(precision=0.22491349480968859, recall=0.09615384615384616, fmeasure=0.13471502590673576), 'rougeLsum': Score(precision=0.22491349480968859, recall=0.09615384615384616, fmeasure=0.13471502590673576)}\n",
      "{'rouge1': Score(precision=0.5777126099706745, recall=0.28592162554426703, fmeasure=0.3825242718446602), 'rouge2': Score(precision=0.16470588235294117, recall=0.08139534883720931, fmeasure=0.10894941634241247), 'rougeL': Score(precision=0.2903225806451613, recall=0.14368650217706821, fmeasure=0.19223300970873786), 'rougeLsum': Score(precision=0.2903225806451613, recall=0.14368650217706821, fmeasure=0.19223300970873786)}\n",
      "{'rouge1': Score(precision=0.6066066066066066, recall=0.3161189358372457, fmeasure=0.4156378600823045), 'rouge2': Score(precision=0.1746987951807229, recall=0.09090909090909091, fmeasure=0.1195876288659794), 'rougeL': Score(precision=0.2672672672672673, recall=0.13928012519561817, fmeasure=0.18312757201646093), 'rougeLsum': Score(precision=0.2672672672672673, recall=0.13928012519561817, fmeasure=0.18312757201646093)}\n",
      "{'rouge1': Score(precision=0.47076023391812866, recall=0.21466666666666667, fmeasure=0.2948717948717949), 'rouge2': Score(precision=0.11730205278592376, recall=0.0534045393858478, fmeasure=0.07339449541284404), 'rougeL': Score(precision=0.28654970760233917, recall=0.13066666666666665, fmeasure=0.17948717948717946), 'rougeLsum': Score(precision=0.28654970760233917, recall=0.13066666666666665, fmeasure=0.17948717948717946)}\n",
      "{'rouge1': Score(precision=0.5682656826568265, recall=0.19469026548672566, fmeasure=0.2900188323917137), 'rouge2': Score(precision=0.16666666666666666, recall=0.056962025316455694, fmeasure=0.08490566037735849), 'rougeL': Score(precision=0.2915129151291513, recall=0.09987357774968394, fmeasure=0.14877589453860643), 'rougeLsum': Score(precision=0.2915129151291513, recall=0.09987357774968394, fmeasure=0.14877589453860643)}\n",
      "{'rouge1': Score(precision=0.44, recall=0.40476190476190477, fmeasure=0.42164599774520856), 'rouge2': Score(precision=0.13443396226415094, recall=0.12364425162689804, fmeasure=0.1288135593220339), 'rougeL': Score(precision=0.24941176470588236, recall=0.22943722943722944, fmeasure=0.23900789177001128), 'rougeLsum': Score(precision=0.24941176470588236, recall=0.22943722943722944, fmeasure=0.23900789177001128)}\n",
      "{'rouge1': Score(precision=0.48773006134969327, recall=0.24386503067484663, fmeasure=0.32515337423312884), 'rouge2': Score(precision=0.13230769230769232, recall=0.06605222734254992, fmeasure=0.08811475409836066), 'rougeL': Score(precision=0.3098159509202454, recall=0.1549079754601227, fmeasure=0.2065439672801636), 'rougeLsum': Score(precision=0.3098159509202454, recall=0.1549079754601227, fmeasure=0.2065439672801636)}\n",
      "{'rouge1': Score(precision=0.5889423076923077, recall=0.3383977900552486, fmeasure=0.4298245614035088), 'rouge2': Score(precision=0.1783132530120482, recall=0.10235131396957123, fmeasure=0.13005272407732862), 'rougeL': Score(precision=0.2668269230769231, recall=0.15331491712707182, fmeasure=0.19473684210526312), 'rougeLsum': Score(precision=0.2668269230769231, recall=0.15331491712707182, fmeasure=0.19473684210526312)}\n",
      "{'rouge1': Score(precision=0.6072507552870091, recall=0.27088948787061995, fmeasure=0.3746505125815471), 'rouge2': Score(precision=0.21212121212121213, recall=0.09446693657219973, fmeasure=0.130718954248366), 'rougeL': Score(precision=0.30513595166163143, recall=0.13611859838274934, fmeasure=0.18825722273998138), 'rougeLsum': Score(precision=0.30513595166163143, recall=0.13611859838274934, fmeasure=0.18825722273998138)}\n",
      "{'rouge1': Score(precision=0.5800524934383202, recall=0.3189033189033189, fmeasure=0.41154562383612664), 'rouge2': Score(precision=0.21052631578947367, recall=0.11560693641618497, fmeasure=0.1492537313432836), 'rougeL': Score(precision=0.30971128608923887, recall=0.17027417027417027, fmeasure=0.21973929236499068), 'rougeLsum': Score(precision=0.30971128608923887, recall=0.17027417027417027, fmeasure=0.21973929236499068)}\n",
      "{'rouge1': Score(precision=0.44, recall=0.3170731707317073, fmeasure=0.36855670103092786), 'rouge2': Score(precision=0.08950617283950617, recall=0.06444444444444444, fmeasure=0.07493540051679587), 'rougeL': Score(precision=0.2, recall=0.14412416851441243, fmeasure=0.1675257731958763), 'rougeLsum': Score(precision=0.2, recall=0.14412416851441243, fmeasure=0.1675257731958763)}\n",
      "{'rouge1': Score(precision=0.22413793103448276, recall=0.12839506172839507, fmeasure=0.16326530612244897), 'rouge2': Score(precision=0.08855291576673865, recall=0.05067985166872682, fmeasure=0.06446540880503145), 'rougeL': Score(precision=0.14439655172413793, recall=0.08271604938271605, fmeasure=0.10518053375196233), 'rougeLsum': Score(precision=0.14439655172413793, recall=0.08271604938271605, fmeasure=0.10518053375196233)}\n",
      "{'rouge1': Score(precision=0.5824175824175825, recall=0.2566585956416465, fmeasure=0.35630252100840343), 'rouge2': Score(precision=0.18732782369146006, recall=0.08242424242424243, fmeasure=0.1144781144781145), 'rougeL': Score(precision=0.3324175824175824, recall=0.14648910411622276, fmeasure=0.2033613445378151), 'rougeLsum': Score(precision=0.3324175824175824, recall=0.14648910411622276, fmeasure=0.2033613445378151)}\n",
      "{'rouge1': Score(precision=0.66875, recall=0.2691823899371069, fmeasure=0.3838565022421525), 'rouge2': Score(precision=0.2476489028213166, recall=0.09949622166246852, fmeasure=0.14195867026055706), 'rougeL': Score(precision=0.35, recall=0.14088050314465408, fmeasure=0.20089686098654708), 'rougeLsum': Score(precision=0.35, recall=0.14088050314465408, fmeasure=0.20089686098654708)}\n",
      "{'rouge1': Score(precision=0.6099706744868035, recall=0.3701067615658363, fmeasure=0.4606866002214839), 'rouge2': Score(precision=0.21470588235294116, recall=0.13012477718360071, fmeasure=0.1620421753607103), 'rougeL': Score(precision=0.2697947214076246, recall=0.16370106761565836, fmeasure=0.20376522702104094), 'rougeLsum': Score(precision=0.2697947214076246, recall=0.16370106761565836, fmeasure=0.20376522702104094)}\n",
      "{'rouge1': Score(precision=0.659400544959128, recall=0.26304347826086955, fmeasure=0.37606837606837606), 'rouge2': Score(precision=0.2185792349726776, recall=0.08705114254624592, fmeasure=0.12451361867704279), 'rougeL': Score(precision=0.33242506811989103, recall=0.13260869565217392, fmeasure=0.18958818958818963), 'rougeLsum': Score(precision=0.33242506811989103, recall=0.13260869565217392, fmeasure=0.18958818958818963)}\n",
      "{'rouge1': Score(precision=0.3743169398907104, recall=0.46440677966101696, fmeasure=0.4145234493192133), 'rouge2': Score(precision=0.12602739726027398, recall=0.1564625850340136, fmeasure=0.1396054628224583), 'rougeL': Score(precision=0.2185792349726776, recall=0.2711864406779661, fmeasure=0.24205748865355523), 'rougeLsum': Score(precision=0.2185792349726776, recall=0.2711864406779661, fmeasure=0.24205748865355523)}\n",
      "{'rouge1': Score(precision=0.5291005291005291, recall=0.35335689045936397, fmeasure=0.42372881355932196), 'rouge2': Score(precision=0.1220159151193634, recall=0.08141592920353982, fmeasure=0.09766454352441613), 'rougeL': Score(precision=0.24867724867724866, recall=0.16607773851590105, fmeasure=0.19915254237288132), 'rougeLsum': Score(precision=0.24867724867724866, recall=0.16607773851590105, fmeasure=0.19915254237288132)}\n",
      "{'rouge1': Score(precision=0.21505376344086022, recall=0.1736111111111111, fmeasure=0.19212295869356388), 'rouge2': Score(precision=0.06896551724137931, recall=0.05565217391304348, fmeasure=0.061597690086621755), 'rougeL': Score(precision=0.14193548387096774, recall=0.11458333333333333, fmeasure=0.12680115273775214), 'rougeLsum': Score(precision=0.14193548387096774, recall=0.11458333333333333, fmeasure=0.12680115273775214)}\n",
      "{'rouge1': Score(precision=0.47474747474747475, recall=0.06695156695156695, fmeasure=0.11735330836454433), 'rouge2': Score(precision=0.12244897959183673, recall=0.017118402282453638, fmeasure=0.030037546933667083), 'rougeL': Score(precision=0.41414141414141414, recall=0.0584045584045584, fmeasure=0.10237203495630462), 'rougeLsum': Score(precision=0.41414141414141414, recall=0.0584045584045584, fmeasure=0.10237203495630462)}\n",
      "{'rouge1': Score(precision=0.5090439276485789, recall=0.34929078014184395, fmeasure=0.41430073606729756), 'rouge2': Score(precision=0.13212435233160622, recall=0.0905861456483126, fmeasure=0.10748155953635405), 'rougeL': Score(precision=0.25839793281653745, recall=0.1773049645390071, fmeasure=0.2103049421661409), 'rougeLsum': Score(precision=0.25839793281653745, recall=0.1773049645390071, fmeasure=0.2103049421661409)}\n",
      "{'rouge1': Score(precision=0.5142857142857142, recall=0.14835164835164835, fmeasure=0.2302771855010661), 'rouge2': Score(precision=0.11483253588516747, recall=0.033012379642365884, fmeasure=0.05128205128205128), 'rougeL': Score(precision=0.30952380952380953, recall=0.08928571428571429, fmeasure=0.13859275053304904), 'rougeLsum': Score(precision=0.30952380952380953, recall=0.08928571428571429, fmeasure=0.13859275053304904)}\n",
      "{'rouge1': Score(precision=0.45938375350140054, recall=0.3416666666666667, fmeasure=0.39187574671445635), 'rouge2': Score(precision=0.10112359550561797, recall=0.07515657620041753, fmeasure=0.08622754491017964), 'rougeL': Score(precision=0.20448179271708683, recall=0.15208333333333332, fmeasure=0.17443249701314217), 'rougeLsum': Score(precision=0.20448179271708683, recall=0.15208333333333332, fmeasure=0.17443249701314217)}\n",
      "{'rouge1': Score(precision=0.5787037037037037, recall=0.33557046979865773, fmeasure=0.4248088360237893), 'rouge2': Score(precision=0.1554524361948956, recall=0.09005376344086022, fmeasure=0.11404255319148936), 'rougeL': Score(precision=0.26157407407407407, recall=0.15167785234899328, fmeasure=0.19201359388275277), 'rougeLsum': Score(precision=0.26157407407407407, recall=0.15167785234899328, fmeasure=0.19201359388275277)}\n",
      "{'rouge1': Score(precision=0.1722689075630252, recall=0.12752721617418353, fmeasure=0.14655942806076855), 'rouge2': Score(precision=0.07789473684210527, recall=0.05763239875389408, fmeasure=0.06624888093106536), 'rougeL': Score(precision=0.12605042016806722, recall=0.09331259720062209, fmeasure=0.10723860589812333), 'rougeLsum': Score(precision=0.12605042016806722, recall=0.09331259720062209, fmeasure=0.10723860589812333)}\n",
      "{'rouge1': Score(precision=0.3844086021505376, recall=0.41210374639769454, fmeasure=0.39777468706536856), 'rouge2': Score(precision=0.09164420485175202, recall=0.09826589595375723, fmeasure=0.09483960948396095), 'rougeL': Score(precision=0.19623655913978494, recall=0.21037463976945245, fmeasure=0.20305980528511822), 'rougeLsum': Score(precision=0.19623655913978494, recall=0.21037463976945245, fmeasure=0.20305980528511822)}\n",
      "{'rouge1': Score(precision=0.5670498084291188, recall=0.32671081677704195, fmeasure=0.41456582633053224), 'rouge2': Score(precision=0.15384615384615385, recall=0.08849557522123894, fmeasure=0.11235955056179776), 'rougeL': Score(precision=0.2681992337164751, recall=0.1545253863134658, fmeasure=0.19607843137254904), 'rougeLsum': Score(precision=0.2681992337164751, recall=0.1545253863134658, fmeasure=0.19607843137254904)}\n",
      "{'rouge1': Score(precision=0.6523809523809524, recall=0.36387782204515273, fmeasure=0.46717817561807334), 'rouge2': Score(precision=0.18138424821002386, recall=0.10106382978723404, fmeasure=0.12980358667805295), 'rougeL': Score(precision=0.2785714285714286, recall=0.1553784860557769, fmeasure=0.19948849104859337), 'rougeLsum': Score(precision=0.2785714285714286, recall=0.1553784860557769, fmeasure=0.19948849104859337)}\n",
      "{'rouge1': Score(precision=0.6564885496183206, recall=0.2042755344418052, fmeasure=0.3115942028985507), 'rouge2': Score(precision=0.2260536398467433, recall=0.07015457788347206, fmeasure=0.10707803992740471), 'rougeL': Score(precision=0.35877862595419846, recall=0.11163895486935867, fmeasure=0.17028985507246377), 'rougeLsum': Score(precision=0.35877862595419846, recall=0.11163895486935867, fmeasure=0.17028985507246377)}\n",
      "{'rouge1': Score(precision=0.6503267973856209, recall=0.28347578347578345, fmeasure=0.3948412698412698), 'rouge2': Score(precision=0.22295081967213115, recall=0.09700427960057062, fmeasure=0.13518886679920478), 'rougeL': Score(precision=0.3137254901960784, recall=0.13675213675213677, fmeasure=0.1904761904761905), 'rougeLsum': Score(precision=0.3137254901960784, recall=0.13675213675213677, fmeasure=0.1904761904761905)}\n",
      "{'rouge1': Score(precision=0.5922330097087378, recall=0.27313432835820894, fmeasure=0.3738508682328907), 'rouge2': Score(precision=0.17207792207792208, recall=0.07922272047832586, fmeasure=0.10849539406345957), 'rougeL': Score(precision=0.2912621359223301, recall=0.13432835820895522, fmeasure=0.18386108273748722), 'rougeLsum': Score(precision=0.2912621359223301, recall=0.13432835820895522, fmeasure=0.18386108273748722)}\n",
      "{'rouge1': Score(precision=0.4768392370572207, recall=0.2390710382513661, fmeasure=0.31847133757961776), 'rouge2': Score(precision=0.15300546448087432, recall=0.07660738714090287, fmeasure=0.10209662716499543), 'rougeL': Score(precision=0.27520435967302453, recall=0.13797814207650272, fmeasure=0.18380345768880801), 'rougeLsum': Score(precision=0.27520435967302453, recall=0.13797814207650272, fmeasure=0.18380345768880801)}\n",
      "{'rouge1': Score(precision=0.5348837209302325, recall=0.47619047619047616, fmeasure=0.5038335158817087), 'rouge2': Score(precision=0.15384615384615385, recall=0.13692946058091288, fmeasure=0.14489571899012074), 'rougeL': Score(precision=0.26744186046511625, recall=0.23809523809523808, fmeasure=0.25191675794085433), 'rougeLsum': Score(precision=0.26744186046511625, recall=0.23809523809523808, fmeasure=0.25191675794085433)}\n",
      "{'rouge1': Score(precision=0.2052980132450331, recall=0.15171288743882544, fmeasure=0.174484052532833), 'rouge2': Score(precision=0.07079646017699115, recall=0.05228758169934641, fmeasure=0.06015037593984962), 'rougeL': Score(precision=0.13245033112582782, recall=0.09787928221859707, fmeasure=0.11257035647279551), 'rougeLsum': Score(precision=0.13245033112582782, recall=0.09787928221859707, fmeasure=0.11257035647279551)}\n",
      "{'rouge1': Score(precision=0.548936170212766, recall=0.21009771986970685, fmeasure=0.30388692579505305), 'rouge2': Score(precision=0.1794871794871795, recall=0.06851549755301795, fmeasure=0.09917355371900827), 'rougeL': Score(precision=0.32340425531914896, recall=0.1237785016286645, fmeasure=0.1790341578327444), 'rougeLsum': Score(precision=0.32340425531914896, recall=0.1237785016286645, fmeasure=0.1790341578327444)}\n",
      "{'rouge1': Score(precision=0.48936170212765956, recall=0.3245967741935484, fmeasure=0.39030303030303026), 'rouge2': Score(precision=0.11280487804878049, recall=0.07474747474747474, fmeasure=0.0899149453219927), 'rougeL': Score(precision=0.2613981762917933, recall=0.17338709677419356, fmeasure=0.2084848484848485), 'rougeLsum': Score(precision=0.2613981762917933, recall=0.17338709677419356, fmeasure=0.2084848484848485)}\n",
      "{'rouge1': Score(precision=0.4693140794223827, recall=0.23465703971119134, fmeasure=0.3128760529482551), 'rouge2': Score(precision=0.11231884057971014, recall=0.05605786618444846, fmeasure=0.07478890229191797), 'rougeL': Score(precision=0.2779783393501805, recall=0.13898916967509026, fmeasure=0.18531889290012032), 'rougeLsum': Score(precision=0.2779783393501805, recall=0.13898916967509026, fmeasure=0.18531889290012032)}\n",
      "{'rouge1': Score(precision=0.5625, recall=0.17342799188640973, fmeasure=0.2651162790697674), 'rouge2': Score(precision=0.2145214521452145, recall=0.06598984771573604, fmeasure=0.10093167701863354), 'rougeL': Score(precision=0.35526315789473684, recall=0.10953346855983773, fmeasure=0.1674418604651163), 'rougeLsum': Score(precision=0.35526315789473684, recall=0.10953346855983773, fmeasure=0.1674418604651163)}\n",
      "{'rouge1': Score(precision=0.4725274725274725, recall=0.29604130808950085, fmeasure=0.36402116402116397), 'rouge2': Score(precision=0.08539944903581267, recall=0.05344827586206897, fmeasure=0.06574761399787911), 'rougeL': Score(precision=0.22527472527472528, recall=0.14113597246127366, fmeasure=0.17354497354497353), 'rougeLsum': Score(precision=0.22527472527472528, recall=0.14113597246127366, fmeasure=0.17354497354497353)}\n",
      "{'rouge1': Score(precision=0.5201342281879194, recall=0.268630849220104, fmeasure=0.3542857142857143), 'rouge2': Score(precision=0.15488215488215487, recall=0.0798611111111111, fmeasure=0.10538373424971362), 'rougeL': Score(precision=0.29194630872483224, recall=0.15077989601386482, fmeasure=0.19885714285714287), 'rougeLsum': Score(precision=0.29194630872483224, recall=0.15077989601386482, fmeasure=0.19885714285714287)}\n",
      "{'rouge1': Score(precision=0.5485564304461942, recall=0.3220338983050847, fmeasure=0.40582524271844655), 'rouge2': Score(precision=0.12105263157894737, recall=0.07098765432098765, fmeasure=0.0894941634241245), 'rougeL': Score(precision=0.2545931758530184, recall=0.14946070878274267, fmeasure=0.18834951456310678), 'rougeLsum': Score(precision=0.2545931758530184, recall=0.14946070878274267, fmeasure=0.18834951456310678)}\n",
      "{'rouge1': Score(precision=0.5586854460093896, recall=0.39403973509933776, fmeasure=0.4621359223300971), 'rouge2': Score(precision=0.1223529411764706, recall=0.08623548922056384, fmeasure=0.10116731517509726), 'rougeL': Score(precision=0.2535211267605634, recall=0.17880794701986755, fmeasure=0.20970873786407765), 'rougeLsum': Score(precision=0.2535211267605634, recall=0.17880794701986755, fmeasure=0.20970873786407765)}\n",
      "{'rouge1': Score(precision=0.4819277108433735, recall=0.0966183574879227, fmeasure=0.16096579476861167), 'rouge2': Score(precision=0.15853658536585366, recall=0.031476997578692496, fmeasure=0.05252525252525254), 'rougeL': Score(precision=0.3253012048192771, recall=0.06521739130434782, fmeasure=0.10865191146881287), 'rougeLsum': Score(precision=0.3253012048192771, recall=0.06521739130434782, fmeasure=0.10865191146881287)}\n",
      "{'rouge1': Score(precision=0.5319148936170213, recall=0.36, fmeasure=0.42938931297709926), 'rouge2': Score(precision=0.12085308056872038, recall=0.08173076923076923, fmeasure=0.09751434034416825), 'rougeL': Score(precision=0.23404255319148937, recall=0.1584, fmeasure=0.18893129770992367), 'rougeLsum': Score(precision=0.23404255319148937, recall=0.1584, fmeasure=0.18893129770992367)}\n",
      "{'rouge1': Score(precision=0.5953177257525084, recall=0.1495798319327731, fmeasure=0.23908663532572194), 'rouge2': Score(precision=0.17114093959731544, recall=0.04289318755256518, fmeasure=0.06859448554135844), 'rougeL': Score(precision=0.34448160535117056, recall=0.0865546218487395, fmeasure=0.13834788448623236), 'rougeLsum': Score(precision=0.34448160535117056, recall=0.0865546218487395, fmeasure=0.13834788448623236)}\n",
      "{'rouge1': Score(precision=0.5504587155963303, recall=0.1791044776119403, fmeasure=0.2702702702702703), 'rouge2': Score(precision=0.18433179723502305, recall=0.059790732436472344, fmeasure=0.09029345372460496), 'rougeL': Score(precision=0.3394495412844037, recall=0.11044776119402985, fmeasure=0.16666666666666669), 'rougeLsum': Score(precision=0.3394495412844037, recall=0.11044776119402985, fmeasure=0.16666666666666669)}\n",
      "{'rouge1': Score(precision=0.6125, recall=0.18434913468773514, fmeasure=0.2834008097165992), 'rouge2': Score(precision=0.20551378446115287, recall=0.061746987951807226, fmeasure=0.09496236247828604), 'rougeL': Score(precision=0.3175, recall=0.09556057185854025, fmeasure=0.14690572585309425), 'rougeLsum': Score(precision=0.3175, recall=0.09556057185854025, fmeasure=0.14690572585309425)}\n",
      "{'rouge1': Score(precision=0.5108695652173914, recall=0.17537313432835822, fmeasure=0.2611111111111111), 'rouge2': Score(precision=0.16483516483516483, recall=0.056179775280898875, fmeasure=0.08379888268156425), 'rougeL': Score(precision=0.3804347826086957, recall=0.13059701492537312, fmeasure=0.19444444444444445), 'rougeLsum': Score(precision=0.3804347826086957, recall=0.13059701492537312, fmeasure=0.19444444444444445)}\n",
      "{'rouge1': Score(precision=0.42363112391930835, recall=0.2474747474747475, fmeasure=0.3124335812964931), 'rouge2': Score(precision=0.08670520231213873, recall=0.050590219224283306, fmeasure=0.06389776357827474), 'rougeL': Score(precision=0.23919308357348704, recall=0.13973063973063973, fmeasure=0.17640807651434645), 'rougeLsum': Score(precision=0.23919308357348704, recall=0.13973063973063973, fmeasure=0.17640807651434645)}\n",
      "{'rouge1': Score(precision=0.5440806045340051, recall=0.2979310344827586, fmeasure=0.3850267379679145), 'rouge2': Score(precision=0.14898989898989898, recall=0.08149171270718232, fmeasure=0.10535714285714284), 'rougeL': Score(precision=0.24685138539042822, recall=0.13517241379310344, fmeasure=0.17468805704099818), 'rougeLsum': Score(precision=0.24685138539042822, recall=0.13517241379310344, fmeasure=0.17468805704099818)}\n",
      "{'rouge1': Score(precision=0.5308988764044944, recall=0.25961538461538464, fmeasure=0.3487084870848709), 'rouge2': Score(precision=0.14929577464788732, recall=0.07290233837689133, fmeasure=0.09796672828096119), 'rougeL': Score(precision=0.2949438202247191, recall=0.14423076923076922, fmeasure=0.19372693726937268), 'rougeLsum': Score(precision=0.2949438202247191, recall=0.14423076923076922, fmeasure=0.19372693726937268)}\n",
      "{'rouge1': Score(precision=0.4895833333333333, recall=0.3375224416517056, fmeasure=0.39957492029755576), 'rouge2': Score(precision=0.10966057441253264, recall=0.07553956834532374, fmeasure=0.08945686900958466), 'rougeL': Score(precision=0.2265625, recall=0.1561938958707361, fmeasure=0.18490967056323063), 'rougeLsum': Score(precision=0.2265625, recall=0.1561938958707361, fmeasure=0.18490967056323063)}\n",
      "{'rouge1': Score(precision=0.49050632911392406, recall=0.21954674220963172, fmeasure=0.30332681017612523), 'rouge2': Score(precision=0.12698412698412698, recall=0.05673758865248227, fmeasure=0.0784313725490196), 'rougeL': Score(precision=0.28164556962025317, recall=0.12606232294617564, fmeasure=0.1741682974559687), 'rougeLsum': Score(precision=0.28164556962025317, recall=0.12606232294617564, fmeasure=0.1741682974559687)}\n",
      "{'rouge1': Score(precision=0.5694117647058824, recall=0.3384615384615385, fmeasure=0.4245614035087719), 'rouge2': Score(precision=0.14150943396226415, recall=0.08403361344537816, fmeasure=0.10544815465729351), 'rougeL': Score(precision=0.2823529411764706, recall=0.16783216783216784, fmeasure=0.2105263157894737), 'rougeLsum': Score(precision=0.2823529411764706, recall=0.16783216783216784, fmeasure=0.2105263157894737)}\n",
      "{'rouge1': Score(precision=0.6182432432432432, recall=0.25811001410437234, fmeasure=0.3641791044776119), 'rouge2': Score(precision=0.17966101694915254, recall=0.0748587570621469, fmeasure=0.10568295114656032), 'rougeL': Score(precision=0.32432432432432434, recall=0.13540197461212977, fmeasure=0.19104477611940301), 'rougeLsum': Score(precision=0.32432432432432434, recall=0.13540197461212977, fmeasure=0.19104477611940301)}\n",
      "{'rouge1': Score(precision=0.63125, recall=0.376865671641791, fmeasure=0.47196261682242996), 'rouge2': Score(precision=0.20689655172413793, recall=0.1233644859813084, fmeasure=0.15456674473067913), 'rougeL': Score(precision=0.309375, recall=0.18470149253731344, fmeasure=0.23130841121495327), 'rougeLsum': Score(precision=0.309375, recall=0.18470149253731344, fmeasure=0.23130841121495327)}\n",
      "{'rouge1': Score(precision=0.5, recall=0.32482993197278914, fmeasure=0.3938144329896907), 'rouge2': Score(precision=0.10236220472440945, recall=0.06643952299829642, fmeasure=0.08057851239669421), 'rougeL': Score(precision=0.24083769633507854, recall=0.1564625850340136, fmeasure=0.18969072164948453), 'rougeLsum': Score(precision=0.24083769633507854, recall=0.1564625850340136, fmeasure=0.18969072164948453)}\n",
      "{'rouge1': Score(precision=0.47160493827160493, recall=0.3472727272727273, fmeasure=0.4), 'rouge2': Score(precision=0.08663366336633663, recall=0.06375227686703097, fmeasure=0.07345225603357818), 'rougeL': Score(precision=0.2222222222222222, recall=0.16363636363636364, fmeasure=0.18848167539267016), 'rougeLsum': Score(precision=0.2222222222222222, recall=0.16363636363636364, fmeasure=0.18848167539267016)}\n",
      "{'rouge1': Score(precision=0.5392405063291139, recall=0.28706199460916443, fmeasure=0.37467018469657), 'rouge2': Score(precision=0.12944162436548223, recall=0.06882591093117409, fmeasure=0.08986784140969163), 'rougeL': Score(precision=0.26582278481012656, recall=0.14150943396226415, fmeasure=0.18469656992084435), 'rougeLsum': Score(precision=0.26582278481012656, recall=0.14150943396226415, fmeasure=0.18469656992084435)}\n",
      "{'rouge1': Score(precision=0.47468354430379744, recall=0.21551724137931033, fmeasure=0.2964426877470355), 'rouge2': Score(precision=0.09523809523809523, recall=0.04316546762589928, fmeasure=0.0594059405940594), 'rougeL': Score(precision=0.24050632911392406, recall=0.10919540229885058, fmeasure=0.15019762845849804), 'rougeLsum': Score(precision=0.24050632911392406, recall=0.10919540229885058, fmeasure=0.15019762845849804)}\n",
      "{'rouge1': Score(precision=0.5974842767295597, recall=0.36259541984732824, fmeasure=0.45130641330166266), 'rouge2': Score(precision=0.1892744479495268, recall=0.1147227533460803, fmeasure=0.14285714285714285), 'rougeL': Score(precision=0.3113207547169811, recall=0.18893129770992367, fmeasure=0.23515439429928744), 'rougeLsum': Score(precision=0.3113207547169811, recall=0.18893129770992367, fmeasure=0.23515439429928744)}\n",
      "{'rouge1': Score(precision=0.5820433436532507, recall=0.33098591549295775, fmeasure=0.42199775533108863), 'rouge2': Score(precision=0.14596273291925466, recall=0.08289241622574955, fmeasure=0.10573678290213723), 'rougeL': Score(precision=0.2631578947368421, recall=0.14964788732394366, fmeasure=0.19079685746352412), 'rougeLsum': Score(precision=0.2631578947368421, recall=0.14964788732394366, fmeasure=0.19079685746352412)}\n",
      "{'rouge1': Score(precision=0.484375, recall=0.3274647887323944, fmeasure=0.3907563025210084), 'rouge2': Score(precision=0.1227154046997389, recall=0.08289241622574955, fmeasure=0.09894736842105262), 'rougeL': Score(precision=0.23958333333333334, recall=0.1619718309859155, fmeasure=0.19327731092436973), 'rougeLsum': Score(precision=0.23958333333333334, recall=0.1619718309859155, fmeasure=0.19327731092436973)}\n",
      "{'rouge1': Score(precision=0.44907407407407407, recall=0.31493506493506496, fmeasure=0.3702290076335878), 'rouge2': Score(precision=0.12064965197215777, recall=0.08455284552845528, fmeasure=0.0994263862332696), 'rougeL': Score(precision=0.25462962962962965, recall=0.17857142857142858, fmeasure=0.2099236641221374), 'rougeLsum': Score(precision=0.25462962962962965, recall=0.17857142857142858, fmeasure=0.2099236641221374)}\n",
      "{'rouge1': Score(precision=0.6867469879518072, recall=0.0800561797752809, fmeasure=0.14339622641509434), 'rouge2': Score(precision=0.3048780487804878, recall=0.035161744022503515, fmeasure=0.06305170239596469), 'rougeL': Score(precision=0.5421686746987951, recall=0.06320224719101124, fmeasure=0.11320754716981134), 'rougeLsum': Score(precision=0.5421686746987951, recall=0.06320224719101124, fmeasure=0.11320754716981134)}\n",
      "{'rouge1': Score(precision=0.3949771689497717, recall=0.211750305997552, fmeasure=0.2756972111553785), 'rouge2': Score(precision=0.13958810068649885, recall=0.07475490196078431, fmeasure=0.09736632083000797), 'rougeL': Score(precision=0.2328767123287671, recall=0.12484700122399021, fmeasure=0.16254980079681275), 'rougeLsum': Score(precision=0.2328767123287671, recall=0.12484700122399021, fmeasure=0.16254980079681275)}\n",
      "{'rouge1': Score(precision=0.5571428571428572, recall=0.2826086956521739, fmeasure=0.37500000000000006), 'rouge2': Score(precision=0.16487455197132617, recall=0.08348457350272233, fmeasure=0.11084337349397591), 'rougeL': Score(precision=0.2892857142857143, recall=0.14673913043478262, fmeasure=0.19471153846153846), 'rougeLsum': Score(precision=0.2892857142857143, recall=0.14673913043478262, fmeasure=0.19471153846153846)}\n",
      "{'rouge1': Score(precision=0.5436893203883495, recall=0.3010752688172043, fmeasure=0.38754325259515576), 'rouge2': Score(precision=0.1411192214111922, recall=0.07806191117092867, fmeasure=0.10051993067590989), 'rougeL': Score(precision=0.2645631067961165, recall=0.14650537634408603, fmeasure=0.18858131487889276), 'rougeLsum': Score(precision=0.2645631067961165, recall=0.14650537634408603, fmeasure=0.18858131487889276)}\n",
      "{'rouge1': Score(precision=0.3183856502242152, recall=0.1953232462173315, fmeasure=0.24211423699914747), 'rouge2': Score(precision=0.08764044943820225, recall=0.05371900826446281, fmeasure=0.06660973526900085), 'rougeL': Score(precision=0.18834080717488788, recall=0.1155433287482806, fmeasure=0.1432225063938619), 'rougeLsum': Score(precision=0.18834080717488788, recall=0.1155433287482806, fmeasure=0.1432225063938619)}\n",
      "{'rouge1': Score(precision=0.579746835443038, recall=0.3453996983408748, fmeasure=0.4328922495274102), 'rouge2': Score(precision=0.1446700507614213, recall=0.08610271903323263, fmeasure=0.10795454545454546), 'rougeL': Score(precision=0.27848101265822783, recall=0.16591251885369532, fmeasure=0.20793950850661624), 'rougeLsum': Score(precision=0.27848101265822783, recall=0.16591251885369532, fmeasure=0.20793950850661624)}\n",
      "{'rouge1': Score(precision=0.5058479532163743, recall=0.21517412935323382, fmeasure=0.30191972076788837), 'rouge2': Score(precision=0.16129032258064516, recall=0.0684931506849315, fmeasure=0.09615384615384615), 'rougeL': Score(precision=0.30116959064327486, recall=0.1281094527363184, fmeasure=0.17975567190226877), 'rougeLsum': Score(precision=0.30116959064327486, recall=0.1281094527363184, fmeasure=0.17975567190226877)}\n",
      "{'rouge1': Score(precision=0.4731182795698925, recall=0.24489795918367346, fmeasure=0.32273838630806845), 'rouge2': Score(precision=0.14028776978417265, recall=0.0724907063197026, fmeasure=0.09558823529411765), 'rougeL': Score(precision=0.26881720430107525, recall=0.1391465677179963, fmeasure=0.1833740831295844), 'rougeLsum': Score(precision=0.26881720430107525, recall=0.1391465677179963, fmeasure=0.1833740831295844)}\n",
      "{'rouge1': Score(precision=0.5979020979020979, recall=0.20046893317702227, fmeasure=0.30026338893766463), 'rouge2': Score(precision=0.13333333333333333, recall=0.04460093896713615, fmeasure=0.06684256816182937), 'rougeL': Score(precision=0.3041958041958042, recall=0.10199296600234467, fmeasure=0.1527655838454785), 'rougeLsum': Score(precision=0.3041958041958042, recall=0.10199296600234467, fmeasure=0.1527655838454785)}\n",
      "{'rouge1': Score(precision=0.7071240105540897, recall=0.17380025940337224, fmeasure=0.27902134305049453), 'rouge2': Score(precision=0.2566137566137566, recall=0.06294613887086307, fmeasure=0.10109431995831163), 'rougeL': Score(precision=0.39050131926121373, recall=0.0959792477302205, fmeasure=0.15408641332639253), 'rougeLsum': Score(precision=0.39050131926121373, recall=0.0959792477302205, fmeasure=0.15408641332639253)}\n",
      "{'rouge1': Score(precision=0.7302325581395349, recall=0.15081652257444764, fmeasure=0.24999999999999997), 'rouge2': Score(precision=0.2757009345794392, recall=0.05673076923076923, fmeasure=0.09409888357256777), 'rougeL': Score(precision=0.42790697674418604, recall=0.08837656099903939, fmeasure=0.1464968152866242), 'rougeLsum': Score(precision=0.42790697674418604, recall=0.08837656099903939, fmeasure=0.1464968152866242)}\n",
      "{'rouge1': Score(precision=0.5625, recall=0.28994845360824745, fmeasure=0.3826530612244898), 'rouge2': Score(precision=0.16040100250626566, recall=0.08258064516129032, fmeasure=0.10902896081771721), 'rougeL': Score(precision=0.2475, recall=0.12757731958762886, fmeasure=0.1683673469387755), 'rougeLsum': Score(precision=0.2475, recall=0.12757731958762886, fmeasure=0.1683673469387755)}\n",
      "{'rouge1': Score(precision=0.5866983372921615, recall=0.3569364161849711, fmeasure=0.4438454627133872), 'rouge2': Score(precision=0.17857142857142858, recall=0.1085383502170767, fmeasure=0.135013501350135), 'rougeL': Score(precision=0.3159144893111639, recall=0.19219653179190752, fmeasure=0.23899371069182393), 'rougeLsum': Score(precision=0.3159144893111639, recall=0.19219653179190752, fmeasure=0.23899371069182393)}\n",
      "{'rouge1': Score(precision=0.578125, recall=0.33154121863799285, fmeasure=0.42141230068337127), 'rouge2': Score(precision=0.16927899686520376, recall=0.09694793536804308, fmeasure=0.1232876712328767), 'rougeL': Score(precision=0.296875, recall=0.17025089605734767, fmeasure=0.2164009111617312), 'rougeLsum': Score(precision=0.296875, recall=0.17025089605734767, fmeasure=0.2164009111617312)}\n",
      "{'rouge1': Score(precision=0.569620253164557, recall=0.20594965675057209, fmeasure=0.3025210084033614), 'rouge2': Score(precision=0.1619047619047619, recall=0.058419243986254296, fmeasure=0.08585858585858587), 'rougeL': Score(precision=0.3322784810126582, recall=0.12013729977116705, fmeasure=0.17647058823529413), 'rougeLsum': Score(precision=0.3322784810126582, recall=0.12013729977116705, fmeasure=0.17647058823529413)}\n",
      "{'rouge1': Score(precision=0.5720720720720721, recall=0.25553319919517103, fmeasure=0.3532684283727399), 'rouge2': Score(precision=0.12217194570135746, recall=0.05443548387096774, fmeasure=0.07531380753138076), 'rougeL': Score(precision=0.2747747747747748, recall=0.1227364185110664, fmeasure=0.16968011126564675), 'rougeLsum': Score(precision=0.2747747747747748, recall=0.1227364185110664, fmeasure=0.16968011126564675)}\n",
      "{'rouge1': Score(precision=0.5667506297229219, recall=0.27915632754342434, fmeasure=0.3740648379052369), 'rouge2': Score(precision=0.1388888888888889, recall=0.06832298136645963, fmeasure=0.09159034138218151), 'rougeL': Score(precision=0.25440806045340053, recall=0.12531017369727046, fmeasure=0.16791354945968412), 'rougeLsum': Score(precision=0.25440806045340053, recall=0.12531017369727046, fmeasure=0.16791354945968412)}\n",
      "{'rouge1': Score(precision=0.5526315789473685, recall=0.25609756097560976, fmeasure=0.35), 'rouge2': Score(precision=0.1436950146627566, recall=0.06648575305291723, fmeasure=0.09090909090909091), 'rougeL': Score(precision=0.30994152046783624, recall=0.14363143631436315, fmeasure=0.1962962962962963), 'rougeLsum': Score(precision=0.30994152046783624, recall=0.14363143631436315, fmeasure=0.1962962962962963)}\n",
      "{'rouge1': Score(precision=0.5863636363636363, recall=0.20739549839228297, fmeasure=0.3064133016627078), 'rouge2': Score(precision=0.1552511415525114, recall=0.05475040257648953, fmeasure=0.08095238095238096), 'rougeL': Score(precision=0.32727272727272727, recall=0.1157556270096463, fmeasure=0.171021377672209), 'rougeLsum': Score(precision=0.32727272727272727, recall=0.1157556270096463, fmeasure=0.171021377672209)}\n",
      "{'rouge1': Score(precision=0.5952380952380952, recall=0.1026694045174538, fmeasure=0.17513134851138354), 'rouge2': Score(precision=0.25301204819277107, recall=0.043209876543209874, fmeasure=0.07381370826010544), 'rougeL': Score(precision=0.4166666666666667, recall=0.07186858316221766, fmeasure=0.12259194395796846), 'rougeLsum': Score(precision=0.4166666666666667, recall=0.07186858316221766, fmeasure=0.12259194395796846)}\n",
      "{'rouge1': Score(precision=0.6035805626598465, recall=0.2773207990599295, fmeasure=0.3800322061191626), 'rouge2': Score(precision=0.1564102564102564, recall=0.07176470588235294, fmeasure=0.09838709677419355), 'rougeL': Score(precision=0.27365728900255754, recall=0.12573443008225618, fmeasure=0.1723027375201288), 'rougeLsum': Score(precision=0.27365728900255754, recall=0.12573443008225618, fmeasure=0.1723027375201288)}\n",
      "{'rouge1': Score(precision=0.5217391304347826, recall=0.3274478330658106, fmeasure=0.40236686390532544), 'rouge2': Score(precision=0.09743589743589744, recall=0.06109324758842444, fmeasure=0.07509881422924902), 'rougeL': Score(precision=0.23017902813299232, recall=0.14446227929373998, fmeasure=0.17751479289940827), 'rougeLsum': Score(precision=0.23017902813299232, recall=0.14446227929373998, fmeasure=0.17751479289940827)}\n",
      "{'rouge1': Score(precision=0.5450236966824644, recall=0.44145873320537427, fmeasure=0.4878048780487805), 'rouge2': Score(precision=0.15914489311163896, recall=0.12884615384615383, fmeasure=0.14240170031880978), 'rougeL': Score(precision=0.25118483412322273, recall=0.2034548944337812, fmeasure=0.22481442205726404), 'rougeLsum': Score(precision=0.25118483412322273, recall=0.2034548944337812, fmeasure=0.22481442205726404)}\n",
      "{'rouge1': Score(precision=0.43636363636363634, recall=0.4528301886792453, fmeasure=0.4444444444444444), 'rouge2': Score(precision=0.12462006079027356, recall=0.12933753943217666, fmeasure=0.12693498452012383), 'rougeL': Score(precision=0.2, recall=0.20754716981132076, fmeasure=0.20370370370370372), 'rougeLsum': Score(precision=0.2, recall=0.20754716981132076, fmeasure=0.20370370370370372)}\n",
      "{'rouge1': Score(precision=0.42857142857142855, recall=0.18053097345132743, fmeasure=0.2540473225404732), 'rouge2': Score(precision=0.16033755274261605, recall=0.0673758865248227, fmeasure=0.09488139825218476), 'rougeL': Score(precision=0.27310924369747897, recall=0.11504424778761062, fmeasure=0.161892901618929), 'rougeLsum': Score(precision=0.27310924369747897, recall=0.11504424778761062, fmeasure=0.161892901618929)}\n",
      "{'rouge1': Score(precision=0.5285714285714286, recall=0.2829827915869981, fmeasure=0.3686176836861768), 'rouge2': Score(precision=0.12186379928315412, recall=0.06513409961685823, fmeasure=0.08489388264669165), 'rougeL': Score(precision=0.2571428571428571, recall=0.13766730401529637, fmeasure=0.1793275217932752), 'rougeLsum': Score(precision=0.2571428571428571, recall=0.13766730401529637, fmeasure=0.1793275217932752)}\n",
      "{'rouge1': Score(precision=0.1836734693877551, recall=0.03237410071942446, fmeasure=0.05504587155963303), 'rouge2': Score(precision=0.1134020618556701, recall=0.01981981981981982, fmeasure=0.03374233128834356), 'rougeL': Score(precision=0.17346938775510204, recall=0.030575539568345324, fmeasure=0.051987767584097865), 'rougeLsum': Score(precision=0.17346938775510204, recall=0.030575539568345324, fmeasure=0.051987767584097865)}\n",
      "{'rouge1': Score(precision=0.5596707818930041, recall=0.22113821138211381, fmeasure=0.317016317016317), 'rouge2': Score(precision=0.13636363636363635, recall=0.05374592833876222, fmeasure=0.07710280373831777), 'rougeL': Score(precision=0.25925925925925924, recall=0.1024390243902439, fmeasure=0.14685314685314685), 'rougeLsum': Score(precision=0.25925925925925924, recall=0.1024390243902439, fmeasure=0.14685314685314685)}\n",
      "{'rouge1': Score(precision=0.5050761421319797, recall=0.49874686716791977, fmeasure=0.501891551071879), 'rouge2': Score(precision=0.13994910941475827, recall=0.13819095477386933, fmeasure=0.1390644753476612), 'rougeL': Score(precision=0.2436548223350254, recall=0.24060150375939848, fmeasure=0.2421185372005044), 'rougeLsum': Score(precision=0.2436548223350254, recall=0.24060150375939848, fmeasure=0.2421185372005044)}\n",
      "{'rouge1': Score(precision=0.4264705882352941, recall=0.35876288659793815, fmeasure=0.38969764837625975), 'rouge2': Score(precision=0.10319410319410319, recall=0.08677685950413223, fmeasure=0.09427609427609426), 'rougeL': Score(precision=0.22794117647058823, recall=0.19175257731958764, fmeasure=0.2082866741321389), 'rougeLsum': Score(precision=0.22794117647058823, recall=0.19175257731958764, fmeasure=0.2082866741321389)}\n",
      "{'rouge1': Score(precision=0.6, recall=0.27432590855803046, fmeasure=0.3765084473049075), 'rouge2': Score(precision=0.19023136246786632, recall=0.08685446009389672, fmeasure=0.1192586623690572), 'rougeL': Score(precision=0.3, recall=0.13716295427901523, fmeasure=0.18825422365245376), 'rougeLsum': Score(precision=0.3, recall=0.13716295427901523, fmeasure=0.18825422365245376)}\n",
      "{'rouge1': Score(precision=0.6317204301075269, recall=0.35179640718562877, fmeasure=0.4519230769230769), 'rouge2': Score(precision=0.215633423180593, recall=0.1199400299850075, fmeasure=0.15414258188824662), 'rougeL': Score(precision=0.3064516129032258, recall=0.17065868263473055, fmeasure=0.21923076923076923), 'rougeLsum': Score(precision=0.3064516129032258, recall=0.17065868263473055, fmeasure=0.21923076923076923)}\n",
      "{'rouge1': Score(precision=0.4708860759493671, recall=0.3059210526315789, fmeasure=0.3708873379860419), 'rouge2': Score(precision=0.09137055837563451, recall=0.05930807248764415, fmeasure=0.07192807192807193), 'rougeL': Score(precision=0.21265822784810126, recall=0.13815789473684212, fmeasure=0.1674975074775673), 'rougeLsum': Score(precision=0.21265822784810126, recall=0.13815789473684212, fmeasure=0.1674975074775673)}\n",
      "{'rouge1': Score(precision=0.493573264781491, recall=0.34532374100719426, fmeasure=0.4063492063492063), 'rouge2': Score(precision=0.12886597938144329, recall=0.09009009009009009, fmeasure=0.10604453870625662), 'rougeL': Score(precision=0.2544987146529563, recall=0.17805755395683454, fmeasure=0.20952380952380953), 'rougeLsum': Score(precision=0.2544987146529563, recall=0.17805755395683454, fmeasure=0.20952380952380953)}\n",
      "{'rouge1': Score(precision=0.22727272727272727, recall=0.030364372469635626, fmeasure=0.05357142857142857), 'rouge2': Score(precision=0.06153846153846154, recall=0.008113590263691683, fmeasure=0.014336917562724014), 'rougeL': Score(precision=0.19696969696969696, recall=0.02631578947368421, fmeasure=0.04642857142857143), 'rougeLsum': Score(precision=0.19696969696969696, recall=0.02631578947368421, fmeasure=0.04642857142857143)}\n",
      "{'rouge1': Score(precision=0.44862155388471175, recall=0.24554183813443073, fmeasure=0.31737588652482274), 'rouge2': Score(precision=0.12814070351758794, recall=0.07005494505494506, fmeasure=0.0905861456483126), 'rougeL': Score(precision=0.20802005012531327, recall=0.11385459533607682, fmeasure=0.14716312056737588), 'rougeLsum': Score(precision=0.20802005012531327, recall=0.11385459533607682, fmeasure=0.14716312056737588)}\n",
      "{'rouge1': Score(precision=0.5211581291759465, recall=0.385502471169687, fmeasure=0.44318181818181823), 'rouge2': Score(precision=0.12053571428571429, recall=0.0891089108910891, fmeasure=0.10246679316888047), 'rougeL': Score(precision=0.22939866369710468, recall=0.1696869851729819, fmeasure=0.19507575757575757), 'rougeLsum': Score(precision=0.22939866369710468, recall=0.1696869851729819, fmeasure=0.19507575757575757)}\n",
      "{'rouge1': Score(precision=0.475, recall=0.3766519823788546, fmeasure=0.4201474201474201), 'rouge2': Score(precision=0.10863509749303621, recall=0.08609271523178808, fmeasure=0.0960591133004926), 'rougeL': Score(precision=0.2222222222222222, recall=0.1762114537444934, fmeasure=0.19656019656019652), 'rougeLsum': Score(precision=0.2222222222222222, recall=0.1762114537444934, fmeasure=0.19656019656019652)}\n",
      "{'rouge1': Score(precision=0.5678233438485805, recall=0.3249097472924188, fmeasure=0.41331802525832384), 'rouge2': Score(precision=0.1962025316455696, recall=0.11211573236889692, fmeasure=0.142692750287687), 'rougeL': Score(precision=0.25236593059936907, recall=0.1444043321299639, fmeasure=0.18369690011481055), 'rougeLsum': Score(precision=0.25236593059936907, recall=0.1444043321299639, fmeasure=0.18369690011481055)}\n",
      "{'rouge1': Score(precision=0.49728260869565216, recall=0.3357798165137615, fmeasure=0.40087623220153346), 'rouge2': Score(precision=0.1444141689373297, recall=0.0974264705882353, fmeasure=0.1163556531284303), 'rougeL': Score(precision=0.22826086956521738, recall=0.15412844036697249, fmeasure=0.18400876232201532), 'rougeLsum': Score(precision=0.22826086956521738, recall=0.15412844036697249, fmeasure=0.18400876232201532)}\n",
      "{'rouge1': Score(precision=0.45110410094637227, recall=0.45396825396825397, fmeasure=0.4525316455696203), 'rouge2': Score(precision=0.12341772151898735, recall=0.12420382165605096, fmeasure=0.12380952380952381), 'rougeL': Score(precision=0.22712933753943218, recall=0.22857142857142856, fmeasure=0.2278481012658228), 'rougeLsum': Score(precision=0.22712933753943218, recall=0.22857142857142856, fmeasure=0.2278481012658228)}\n",
      "{'rouge1': Score(precision=0.5346153846153846, recall=0.3041575492341357, fmeasure=0.38772663877266395), 'rouge2': Score(precision=0.16988416988416988, recall=0.09649122807017543, fmeasure=0.12307692307692308), 'rougeL': Score(precision=0.29615384615384616, recall=0.16849015317286653, fmeasure=0.21478382147838215), 'rougeLsum': Score(precision=0.29615384615384616, recall=0.16849015317286653, fmeasure=0.21478382147838215)}\n",
      "{'rouge1': Score(precision=0.5972222222222222, recall=0.25443786982248523, fmeasure=0.35684647302904565), 'rouge2': Score(precision=0.1813953488372093, recall=0.07707509881422925, fmeasure=0.10818307905686547), 'rougeL': Score(precision=0.30092592592592593, recall=0.1282051282051282, fmeasure=0.1798063623789765), 'rougeLsum': Score(precision=0.30092592592592593, recall=0.1282051282051282, fmeasure=0.1798063623789765)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "scores = []\n",
    "for i in range(len(X_test)):\n",
    "    u = \"Case appeal: [\"+X_test.tolist()[i]+\"]\"\n",
    "    input_ids = tokenizer.encode(u,truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    generated_verdict = model.generate(input_ids, max_length=512, num_return_sequences=1)[0]\n",
    "\n",
    "    decoded_text = tokenizer.decode(generated_verdict.cpu(), skip_special_tokens=True)\n",
    "\n",
    "    scorer=rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL','rougeLsum'])\n",
    "    scores1 = scorer.score(y_test.tolist()[i],decoded_text)\n",
    "    scores.append(scores1)\n",
    "    print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "geP8dklN8x-S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geP8dklN8x-S",
    "outputId": "8edcbfb2-08f9-47a6-edce-f6a61214e7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rouge1 Precision: 0.5188299115380309\n",
      "Average Rouge1 Recall: 0.27405336924176965\n",
      "Average Rouge1 F-Measure: 0.3480278489893209\n"
     ]
    }
   ],
   "source": [
    "total_precision = 0\n",
    "total_recall=0\n",
    "total_fmeasure=0\n",
    "count = 0\n",
    "\n",
    "for score_dict in scores:\n",
    "    if \"rouge1\" in score_dict:\n",
    "        total_precision += score_dict[\"rouge1\"].precision\n",
    "        total_recall += score_dict[\"rouge1\"].recall\n",
    "        total_fmeasure += score_dict[\"rouge1\"].fmeasure\n",
    "        count += 1\n",
    "\n",
    "average_precision = total_precision / count if count > 0 else 0\n",
    "average_recall = total_recall / count if count > 0 else 0\n",
    "average_fmeasure = total_fmeasure / count if count > 0 else 0\n",
    "\n",
    "print(\"Average Rouge1 Precision:\", average_precision)\n",
    "print(\"Average Rouge1 Recall:\", average_recall)\n",
    "print(\"Average Rouge1 F-Measure:\", average_fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "D7J0dGXkEpYO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7J0dGXkEpYO",
    "outputId": "bb22d0a2-5215-4ffa-ba4e-27b7b623cbb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rouge2 Precision: 0.14812283118982691\n",
      "Average Rouge2 Recall: 0.07589086507374372\n",
      "Average Rouge2 F-Measure: 0.09711551888467784\n"
     ]
    }
   ],
   "source": [
    "total_precision = 0\n",
    "total_recall=0\n",
    "total_fmeasure=0\n",
    "count = 0\n",
    "\n",
    "for score_dict in scores:\n",
    "    if \"rouge2\" in score_dict:\n",
    "        total_precision += score_dict[\"rouge2\"].precision\n",
    "        total_recall += score_dict[\"rouge2\"].recall\n",
    "        total_fmeasure += score_dict[\"rouge2\"].fmeasure\n",
    "        count += 1\n",
    "\n",
    "average_precision = total_precision / count if count > 0 else 0\n",
    "average_recall = total_recall / count if count > 0 else 0\n",
    "average_fmeasure = total_fmeasure / count if count > 0 else 0\n",
    "\n",
    "print(\"Average Rouge2 Precision:\", average_precision)\n",
    "print(\"Average Rouge2 Recall:\", average_recall)\n",
    "print(\"Average Rouge2 F-Measure:\", average_fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "TUS-ATtjGEgq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUS-ATtjGEgq",
    "outputId": "9b6bdac2-3f45-43f4-b349-b0dde57757ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RougeL Precision: 0.2736098138632765\n",
      "Average RougeL Recall: 0.14040438306097427\n",
      "Average RougeL F-Measure: 0.17940789518890388\n"
     ]
    }
   ],
   "source": [
    "total_precision = 0\n",
    "total_recall=0\n",
    "total_fmeasure=0\n",
    "count = 0\n",
    "\n",
    "for score_dict in scores:\n",
    "    if \"rougeL\" in score_dict:\n",
    "        total_precision += score_dict[\"rougeL\"].precision\n",
    "        total_recall += score_dict[\"rougeL\"].recall\n",
    "        total_fmeasure += score_dict[\"rougeL\"].fmeasure\n",
    "        count += 1\n",
    "\n",
    "average_precision = total_precision / count if count > 0 else 0\n",
    "average_recall = total_recall / count if count > 0 else 0\n",
    "average_fmeasure = total_fmeasure / count if count > 0 else 0\n",
    "\n",
    "print(\"Average RougeL Precision:\", average_precision)\n",
    "print(\"Average RougeL Recall:\", average_recall)\n",
    "print(\"Average RougeL F-Measure:\", average_fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "It9xcyRPGOLM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "It9xcyRPGOLM",
    "outputId": "a492159a-771d-4d2e-8e32-ba854d4ee19f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RougeLsum Precision: 0.26862981484739407\n",
      "Average RougeLsum Recall: 0.15002831546328851\n",
      "Average RougeLsum F-Measure: 0.18690166593355823\n"
     ]
    }
   ],
   "source": [
    "total_precision = 0\n",
    "total_recall=0\n",
    "total_fmeasure=0\n",
    "count = 0\n",
    "\n",
    "for score_dict in scores:\n",
    "    if \"rougeLsum\" in score_dict:\n",
    "        total_precision += score_dict[\"rougeLsum\"].precision\n",
    "        total_recall += score_dict[\"rougeLsum\"].recall\n",
    "        total_fmeasure += score_dict[\"rougeLsum\"].fmeasure\n",
    "        count += 1\n",
    "\n",
    "average_precision = total_precision / count if count > 0 else 0\n",
    "average_recall = total_recall / count if count > 0 else 0\n",
    "average_fmeasure = total_fmeasure / count if count > 0 else 0\n",
    "\n",
    "print(\"Average RougeLsum Precision:\", average_precision)\n",
    "print(\"Average RougeLsum Recall:\", average_recall)\n",
    "print(\"Average RougeLsum F-Measure:\", average_fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yUI8qQMZ4lgY",
   "metadata": {
    "id": "yUI8qQMZ4lgY"
   },
   "source": [
    "#### Meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1DTigm8I4_jl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1DTigm8I4_jl",
    "outputId": "d7cc8ed6-033e-4034-c6be-ed7be67f1a89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "rXRyFpcf5uP1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXRyFpcf5uP1",
    "outputId": "ae5d6c51-8b18-4b5b-f0f5-666080c47a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1971]\n",
      "[0.1971, 0.2183]\n",
      "[0.1971, 0.2183, 0.3153]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789, 0.2522]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789, 0.2522, 0.3159]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789, 0.2522, 0.3159, 0.2435]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789, 0.2522, 0.3159, 0.2435, 0.2026]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789, 0.2522, 0.3159, 0.2435, 0.2026, 0.2362]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789, 0.2522, 0.3159, 0.2435, 0.2026, 0.2362, 0.286]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789, 0.2522, 0.3159, 0.2435, 0.2026, 0.2362, 0.286, 0.1621]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789, 0.2522, 0.3159, 0.2435, 0.2026, 0.2362, 0.286, 0.1621, 0.2436]\n",
      "[0.1971, 0.2183, 0.3153, 0.2586, 0.2575, 0.2342, 0.2757, 0.3173, 0.2567, 0.2729, 0.2443, 0.3166, 0.2122, 0.2748, 0.3107, 0.3736, 0.2236, 0.3455, 0.363, 0.4135, 0.2652, 0.3003, 0.2585, 0.3045, 0.2948, 0.2756, 0.2409, 0.3075, 0.1506, 0.2192, 0.2778, 0.2268, 0.1769, 0.2198, 0.2056, 0.2047, 0.2851, 0.2637, 0.1664, 0.2565, 0.269, 0.2531, 0.2627, 0.2412, 0.3408, 0.2008, 0.2555, 0.3159, 0.286, 0.2452, 0.2638, 0.1943, 0.2447, 0.3029, 0.3493, 0.2168, 0.2418, 0.1305, 0.2621, 0.3157, 0.1632, 0.1842, 0.2788, 0.2879, 0.2711, 0.2779, 0.247, 0.1932, 0.2345, 0.2885, 0.229, 0.2977, 0.1155, 0.2437, 0.2378, 0.2581, 0.2464, 0.221, 0.2577, 0.2548, 0.2595, 0.2981, 0.1755, 0.2113, 0.3596, 0.2632, 0.203, 0.2402, 0.2981, 0.2475, 0.2471, 0.2375, 0.2901, 0.2538, 0.322, 0.3049, 0.1482, 0.2448, 0.2125, 0.2193, 0.1993, 0.2449, 0.2685, 0.1511, 0.252, 0.278, 0.2378, 0.1939, 0.1965, 0.3376, 0.2626, 0.2895, 0.2798, 0.2829, 0.2137, 0.2647, 0.2334, 0.2212, 0.3229, 0.2797, 0.2554, 0.2618, 0.1986, 0.2486, 0.2292, 0.2474, 0.2196, 0.2033, 0.2156, 0.2573, 0.3264, 0.2111, 0.2789, 0.2522, 0.3159, 0.2435, 0.2026, 0.2362, 0.286, 0.1621, 0.2436, 0.1957]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import meteor\n",
    "from nltk.tokenize import word_tokenize\n",
    "scores = []\n",
    "for i in range(len(X_test)):\n",
    "    u = \"Case appeal: [\"+X_test.tolist()[i]+\"]\"\n",
    "    input_ids = tokenizer.encode(u,truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    generated_verdict = model.generate(input_ids, max_length=512, num_return_sequences=1)[0]\n",
    "\n",
    "    decoded_text = tokenizer.decode(generated_verdict.cpu(), skip_special_tokens=True)\n",
    "    reference = word_tokenize(y_test.iloc[i])\n",
    "    decoded_text = word_tokenize(decoded_text)\n",
    "    meteor_score = round(meteor([decoded_text],reference), 4)\n",
    "    scores.append(meteor_score)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "DKyDTGwu-zvo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "DKyDTGwu-zvo",
    "outputId": "19e9d119-7922-488c-f171-586a34d8f712"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb05da2271d6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LQCVs8qy9GLe",
   "metadata": {
    "id": "LQCVs8qy9GLe"
   },
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zljd3ZdOCpID",
   "metadata": {
    "id": "Zljd3ZdOCpID"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Ic9ZjfueggVb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ic9ZjfueggVb",
    "outputId": "366fc172-c511-434a-bffc-2350174abd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "DXQ5-tX_RAF9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXQ5-tX_RAF9",
    "outputId": "60aab57a-fb8b-473b-f041-4460826b1925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qrrpBCAFOHob",
   "metadata": {
    "id": "qrrpBCAFOHob"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5Tokenizer, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "E6JzwfmPigCW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6JzwfmPigCW",
    "outputId": "683d9ec3-2085-4afc-e10f-6d5363de3d96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory where you want to save the model\n",
    "model_dir = \"/content/drive/MyDrive/t5-small-model\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_dir, model_max_length=2048)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2TYPeoIVgGVW",
   "metadata": {
    "id": "2TYPeoIVgGVW"
   },
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Specify the directory where you want to save the model\n",
    "model_dir = \"/content/drive/MyDrive/Pegasus_model\"\n",
    "\n",
    "# Load the model\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_dir,model_max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "T-r3ISjMQfDk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-r3ISjMQfDk",
    "outputId": "8c27c2f1-a0f3-4801-e0fa-73b4690ca2ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fyC2mwagGS3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fyC2mwagGS3",
    "outputId": "5a308a5e-d758-475c-b44f-851f46782416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,    37, 20512, 24910, 19664,  8667,   446, 13985,    23,    45,\n",
      "          160,  4311,    44,     8,    97,    13,   160,  3689,     3,     5,\n",
      "           37, 23698, 19664,     8, 10394,    13, 18216, 16708,   138,    44,\n",
      "            8,    97,    13,   160,  3689,     3,     5,   283,     7,   446,\n",
      "        13985,    23,    47,  1426,    15,    26,    12,    20,  1788,  4128,\n",
      "           57,  6775,    13,     8,   349,    30,     8,  9808,    13,   160,\n",
      "        23585,  4672,  3108,     3,     5,  5568,     3, 17864,   188,   405,\n",
      "           59,  1581,    12,     3,     9, 16708,   138,     3,  6475,   283,\n",
      "            7,   446, 13985,    23,     3,     5,     1])\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Case appeal: [The appeal concerns the dismissal of Ms Jhuti, the appellant, from her employment by Royal Mail Group Ltd (“the company”). The key question of law that it raises is as follows: in a claim for unfair dismissal under Part X of the Employment Rights Act 1996 (“the Act”), can the reason for the dismissal be other than that given to the employee by the employer’s appointed decision-maker? The facts found by the employment tribunal (“the tribunal”) in this case included the following. During her trial period, Ms Jhuti made ‘protected disclosures’ under section 43A of the Act, commonly described as whistleblowing. Her line manager’s response was to pretend that her performance was inadequate, including by bullying her and by creating, in emails and otherwise, a false picture of her performance. The company appointed another employee to decide whether Ms Jhuti should be dismissed. Ms Jhuti, who had in the meantime been signed off work for work-related stress, anxiety and depression, was unable to present her case to the decision-maker in meetings or otherwise. Having no reason to doubt the truthfulness of the material indicative of Ms Jhuti’s inadequate performance, the decision-maker decided that she should be dismissed for that reason. Ms Jhuti brought two complaints in the tribunal. The first complaint (on which nothing in the present appeal turns directly) was that, contrary to section 47B(1) of the Act, she had been subjected to detriments by acts of the company done on the ground of her whistleblowing. The second complaint was that her dismissal was unfair under section 103A, which provides that a dismissal is unfair ‘if the reason (or, if more than one, the principal reason) for the dismissal is that the employee made a protected disclosure’. The tribunal dismissed this second complaint. It found that, as the decision-maker had dismissed her on the ground of a genuine belief that her performance had been inadequate, the reason for dismissal was her performance and so section 103A did not apply. The Employment Appeal Tribunal (“the EAT”) reversed this decision, holding that the reason for dismissal was the making of the protected disclosures. The Court of Appeal allowed the company’s appeal against the EAT’s decision and reinstated the tribunal’s dismissal of the complaint of unfair dismissal. It held that a tribunal required to determine the reason for dismissal under section 103A was obliged to consider only the mental processes of the employer’s authorised decision-maker. Ms Jhuti appealed to the Supreme Court.]\\nVerdict:\"\n",
    "input_ids = tokenizer.encode(input_prompt,truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "input_ids = input_ids.to(model.device)\n",
    "\n",
    "generated_verdict = model.generate(input_ids, max_length=1024, num_return_sequences=1)[0]\n",
    "print(generated_verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8FM6-LLqgGQG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FM6-LLqgGQG",
    "outputId": "a4b5fe3b-ff47-4372-df22-f4ffe7f8067b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Employment Tribunal dismissed Mrs Jhuti from her employment at the time of her trial. The tribunal dismissed the complaint of unfair dismissal at the time of her trial. Ms Jhuti was subjected to detriments by acts of the company on the grounds of her whistleblowing. Section 103A does not apply to a dismissal involving Ms Jhuti.\n"
     ]
    }
   ],
   "source": [
    "# Convert the tensor back to CPU if necessary\n",
    "decoded_text = tokenizer.decode(generated_verdict.cpu(), skip_special_tokens=True)\n",
    "\n",
    "# Print the generated verdict\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aASb9KVYvAMa",
   "metadata": {
    "id": "aASb9KVYvAMa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LhIMtsVrvAKD",
   "metadata": {
    "id": "LhIMtsVrvAKD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sJSn_tQxvAH1",
   "metadata": {
    "id": "sJSn_tQxvAH1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cgjT2DOBvAFq",
   "metadata": {
    "id": "cgjT2DOBvAFq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hu4q4p7svADh",
   "metadata": {
    "id": "Hu4q4p7svADh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3NIl_isXvABU",
   "metadata": {
    "id": "3NIl_isXvABU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SDmuPZ_fu__O",
   "metadata": {
    "id": "SDmuPZ_fu__O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2vhp9MoOu_9A",
   "metadata": {
    "id": "2vhp9MoOu_9A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Al6-ObIhu_6x",
   "metadata": {
    "id": "Al6-ObIhu_6x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zSEXqQdQu_4d",
   "metadata": {
    "id": "zSEXqQdQu_4d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6p9wpzXmu_2O",
   "metadata": {
    "id": "6p9wpzXmu_2O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vD33c0wXu_z1",
   "metadata": {
    "id": "vD33c0wXu_z1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dqRLiTmqu_xZ",
   "metadata": {
    "id": "dqRLiTmqu_xZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "pQtNX93GuTQo",
    "3ZUeVdT9kTdo",
    "F0KiquijDwt4"
   ],
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
